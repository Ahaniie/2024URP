{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc08077",
   "metadata": {
    "papermill": {
     "duration": 0.008184,
     "end_time": "2024-03-18T13:10:42.279133",
     "exception": false,
     "start_time": "2024-03-18T13:10:42.270949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Stuff & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80caa367",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:42.297330Z",
     "iopub.status.busy": "2024-03-18T13:10:42.297029Z",
     "iopub.status.idle": "2024-03-18T13:10:50.995995Z",
     "shell.execute_reply": "2024-03-18T13:10:50.994940Z"
    },
    "papermill": {
     "duration": 8.710986,
     "end_time": "2024-03-18T13:10:50.998486",
     "exception": false,
     "start_time": "2024-03-18T13:10:42.287500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3bd6dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.017736Z",
     "iopub.status.busy": "2024-03-18T13:10:51.017143Z",
     "iopub.status.idle": "2024-03-18T13:10:51.107888Z",
     "shell.execute_reply": "2024-03-18T13:10:51.106932Z"
    },
    "papermill": {
     "duration": 0.102383,
     "end_time": "2024-03-18T13:10:51.109927",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.007544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904f3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c122a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.129159Z",
     "iopub.status.busy": "2024-03-18T13:10:51.128887Z",
     "iopub.status.idle": "2024-03-18T13:10:51.132703Z",
     "shell.execute_reply": "2024-03-18T13:10:51.131901Z"
    },
    "papermill": {
     "duration": 0.015454,
     "end_time": "2024-03-18T13:10:51.134564",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.119110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 64\n",
    "block_frame = 9\n",
    "frame_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2c4ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.152594Z",
     "iopub.status.busy": "2024-03-18T13:10:51.152320Z",
     "iopub.status.idle": "2024-03-18T13:10:51.156215Z",
     "shell.execute_reply": "2024-03-18T13:10:51.155441Z"
    },
    "papermill": {
     "duration": 0.014977,
     "end_time": "2024-03-18T13:10:51.158055",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.143078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d1d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.177027Z",
     "iopub.status.busy": "2024-03-18T13:10:51.176780Z",
     "iopub.status.idle": "2024-03-18T13:10:51.181166Z",
     "shell.execute_reply": "2024-03-18T13:10:51.180357Z"
    },
    "papermill": {
     "duration": 0.015692,
     "end_time": "2024-03-18T13:10:51.183090",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.167398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # transforms.Lambda(lambda x: x / 255.),\n",
    "    transforms.Resize((RESOLUTION, RESOLUTION))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c986f9",
   "metadata": {
    "papermill": {
     "duration": 0.008199,
     "end_time": "2024-03-18T13:10:51.199759",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.191560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4673b989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.218056Z",
     "iopub.status.busy": "2024-03-18T13:10:51.217793Z",
     "iopub.status.idle": "2024-03-18T13:10:51.224015Z",
     "shell.execute_reply": "2024-03-18T13:10:51.223123Z"
    },
    "papermill": {
     "duration": 0.01771,
     "end_time": "2024-03-18T13:10:51.225987",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.208277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_image_from_list(__images, __labels, __count):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for _i in range(__count[0] * __count[1]):\n",
    "        plt.subplot(__count[0], __count[1], _i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        _img = __images[_i].to(\"cpu\").numpy()\n",
    "        plt.imshow(_img, cmap= \"gray\")\n",
    "        plt.xlabel(__labels[_i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74a577b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.244593Z",
     "iopub.status.busy": "2024-03-18T13:10:51.244285Z",
     "iopub.status.idle": "2024-03-18T13:10:51.249956Z",
     "shell.execute_reply": "2024-03-18T13:10:51.249074Z"
    },
    "papermill": {
     "duration": 0.017468,
     "end_time": "2024-03-18T13:10:51.251954",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.234486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_randomly_form_dataset(__dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _ in range(0, 25):\n",
    "        index = torch.randint(0, len(__dataset), (1,)).item()\n",
    "        image, label = __dataset[index]\n",
    "        images.append(image[:,0].squeeze())\n",
    "        labels.append(__dataset.label_id2str(label.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (5, 5))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf33a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequence_form_dataset(__dataset, index):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(0, block_frame):\n",
    "        image, label = __dataset[index]\n",
    "        images.append(image[:,i].squeeze())\n",
    "        labels.append(__dataset.label_id2str(label.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (1, block_frame))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da33dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_randomly_form_dataset_model(__dataset, __model):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _ in range(0, 25):\n",
    "        index = torch.randint(0, len(__dataset), (1,)).item()\n",
    "        image, label = __dataset[index]\n",
    "        result = model(image.unsqueeze(0).to(device)).cpu().argmax()\n",
    "        images.append(image[:,0].squeeze())\n",
    "        labels.append(__dataset.label_id2str(label.item()) + \" -> \" + __dataset.label_id2str(result.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (5, 5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ae748",
   "metadata": {
    "papermill": {
     "duration": 0.008208,
     "end_time": "2024-03-18T13:10:51.296122",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.287914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75883e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.314167Z",
     "iopub.status.busy": "2024-03-18T13:10:51.313885Z",
     "iopub.status.idle": "2024-03-18T13:10:51.318124Z",
     "shell.execute_reply": "2024-03-18T13:10:51.317353Z"
    },
    "papermill": {
     "duration": 0.015519,
     "end_time": "2024-03-18T13:10:51.320057",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.304538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_SOURCE = {\n",
    "       \"Abuse\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Abuse\",\n",
    "    \"Arrest\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Arrest\",\n",
    "    \"Arson\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Arson\",\n",
    "    \"Assault\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Assault\",\n",
    "    \"Burglary\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Burglary\",\n",
    "    \"Explosion\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Explosion\",\n",
    "    \"Fighting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Fighting\",   \n",
    "    \"RoadAccidents\":\"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/RoadAccidents\",\n",
    "    \"Robbery\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/Robbery\",\n",
    "    \"Shooting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/Shooting\",\n",
    "    \"Shoplifting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Shoplifting\",\n",
    "    \"Stealing\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Stealing\",\n",
    "    \"Vandalism\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Vandalism\",\n",
    "    \"Normal\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Training-Normal-Videos-Part-1/Training-Normal-Videos-Part-1\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85661581",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly_Train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m txt_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly_Train.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m train_dic \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtxt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly_Train.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "txt_train = \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly_Train.txt\"\n",
    "train_dic = {}\n",
    "with open(txt_train, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('/')\n",
    "        train_dic[parts[1]] = parts[0]\n",
    "    \n",
    "txt_test = \"/home/yhwoo7/Anomaly-Detection-Dataset/Temporal_Anomaly_Annotation_for_Testing_Videos.txt\"\n",
    "test_dic = {}\n",
    "with open(txt_test, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        video_name = parts[0]\n",
    "        event_name = parts[1]\n",
    "        start_frame1 = int(parts[2])\n",
    "        end_frame1 = int(parts[3])\n",
    "        start_frame2 = int(parts[4])\n",
    "        end_frame2 = int(parts[5])\n",
    "        test_dic[video_name] = [event_name, start_frame1, end_frame1, start_frame2, end_frame2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for directory_path in TEST_SOURCE.items():\n",
    "# 경로가 존재하는지 확인합니다.\n",
    "    if os.path.exists(directory_path[1]):\n",
    "        print(\"경로가 존재합니다.\")\n",
    "        # 디렉터리 내의 파일 목록을 출력합니다.\n",
    "        print(\"디렉터리 내용:\", os.listdir(directory_path[1]))\n",
    "    else:\n",
    "        print(\"지정된 경로를 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d06374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.338518Z",
     "iopub.status.busy": "2024-03-18T13:10:51.338184Z",
     "iopub.status.idle": "2024-03-18T13:10:51.352085Z",
     "shell.execute_reply": "2024-03-18T13:10:51.351221Z"
    },
    "papermill": {
     "duration": 0.025363,
     "end_time": "2024-03-18T13:10:51.353938",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.328575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrimeDataset(Dataset):\n",
    "    train = True\n",
    "    def __init__(self, __train=True):\n",
    "        train = __train\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        self._frame_interval = frame_interval\n",
    "        \n",
    "        print(f\"Loading {'train' if train else 'test' } dataset...\")\n",
    "\n",
    "        if train:\n",
    "            for label, data_path in DATA_SOURCE.items():\n",
    "                print(f\"Loading Label {label}...\")\n",
    "            \n",
    "                for file in tqdm(os.listdir(data_path)):\n",
    "                    if file.endswith(\".mp4\") and file in train_dic:\n",
    "                        path = os.path.join(data_path, file)\n",
    "                        data, labels = self._parse_file(path, label)\n",
    "                    \n",
    "                        self._data.extend(data)\n",
    "                        self._labels.extend(labels)\n",
    "        else:\n",
    "            for label, data_path in DATA_SOURCE.items():\n",
    "                print(f\"Loading Label {label}...\")\n",
    "            \n",
    "                for file in tqdm(os.listdir(data_path)):\n",
    "                    if file.endswith(\".mp4\") and file in test_dic:\n",
    "                        path = os.path.join(data_path, file)\n",
    "                        data, labels = self._parse_file(path, label)\n",
    "                \n",
    "                    \n",
    "                        self._data.extend(data)\n",
    "                        self._labels.extend(labels)\n",
    "                    \n",
    "        print(f\"Finished loading {'train' if train else 'test' } dataset... Loaded  {len(self._data)} images.\")\n",
    "    \n",
    "    def _parse_file(self, __path, __label):\n",
    "        if not os.path.exists(__path):\n",
    "            return [], []\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        cap = cv2.VideoCapture(__path)\n",
    "        \n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        success, image = cap.read()\n",
    "        while success:\n",
    "            try:\n",
    "                if True:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "                    Y, U, V = cv2.split(image)\n",
    "                    image = transformer(Y)\n",
    "                    frames.append(image)\n",
    "                    \n",
    "                    if len(frames) == block_frame:\n",
    "                        if train:\n",
    "                            stacked_frames = torch.stack(frames)\n",
    "                            stacked_frames = stacked_frames.permute(1, 0, 2, 3)\n",
    "                            data.append(stacked_frames)\n",
    "                            labels.append(__label)\n",
    "                            frames = frames[3:]\n",
    "                        else:\n",
    "                            stacked_frames = torch.stack(frames)\n",
    "                            stacked_frames = stacked_frames.permute(1, 0, 2, 3)\n",
    "                            data.append(stacked_frames)\n",
    "                            filepath_list = __path.split('\\\\')\n",
    "                            event_name, start_frame1, end_frame1, start_frame2, end_frame2 = test_dic[filepath_list[-1]]\n",
    "                            if (frame_count >= start_frame1 and frame_count <= end_frame1) or (frame_count >= start_frame2 and frame_count <= end_frame2):\n",
    "                                labels.append(__label)\n",
    "                            else:\n",
    "                                labels.append(\"Normal\")\n",
    "                            frames = frames[3:]\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {__path}: {e}\")\n",
    "            \n",
    "            count = 0\n",
    "            while success and count < self._frame_interval:\n",
    "                success, image = cap.read()\n",
    "                frame_count +=1\n",
    "                count += 1\n",
    "                \n",
    "        if len(frames) > 0 and len(frames) < block_frame:\n",
    "            while len(frames) < block_frame:\n",
    "                frames.append(frames[-1])\n",
    "            stacked_frames = torch.stack(frames)\n",
    "            stacked_frames = stacked_frames.permute(1, 0, 2, 3)\n",
    "            data.append(stacked_frames)\n",
    "            if train:\n",
    "                labels.append(__label)\n",
    "            else:\n",
    "                if (frame_count >= start_frame1 and frame_count <= end_frame1) or (frame_count >= start_frame2 and frame_count <= end_frame2):\n",
    "                    labels.append(__label)\n",
    "                else:\n",
    "                    labels.append(\"Normal\")       \n",
    "        return data, labels\n",
    "        \n",
    "    \n",
    "    def label_str2id(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels).index(__label)\n",
    "\n",
    "    def label_id2str(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels)[__label]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, __idx):\n",
    "        data = self._data[__idx]\n",
    "        label = self._labels[__idx]\n",
    "        return data, torch.tensor([self.label_str2id(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eab9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.371922Z",
     "iopub.status.busy": "2024-03-18T13:10:51.371655Z",
     "iopub.status.idle": "2024-03-18T13:21:45.090780Z",
     "shell.execute_reply": "2024-03-18T13:21:45.089667Z"
    },
    "papermill": {
     "duration": 653.730762,
     "end_time": "2024-03-18T13:21:45.093180",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.362418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "train_dataset = CrimeDataset(True)\n",
    "train = False\n",
    "test_dataset = CrimeDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63356b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:45.223016Z",
     "iopub.status.busy": "2024-03-18T13:21:45.222687Z",
     "iopub.status.idle": "2024-03-18T13:21:46.551732Z",
     "shell.execute_reply": "2024-03-18T13:21:46.550759Z"
    },
    "papermill": {
     "duration": 1.421088,
     "end_time": "2024-03-18T13:21:46.557862",
     "exception": false,
     "start_time": "2024-03-18T13:21:45.136774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_dataset._data[0].shape)\n",
    "print(len(train_dataset))\n",
    "plot_randomly_form_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157940e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:46.663279Z",
     "iopub.status.busy": "2024-03-18T13:21:46.662934Z",
     "iopub.status.idle": "2024-03-18T13:21:46.667944Z",
     "shell.execute_reply": "2024-03-18T13:21:46.667044Z"
    },
    "papermill": {
     "duration": 0.060875,
     "end_time": "2024-03-18T13:21:46.669971",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.609096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "# print(len(train_dataset._data))\n",
    "# print(len(train_dataset._labels))\n",
    "print(train_dataset._data[0].shape)\n",
    "print(train_dataset[0][1])\n",
    "print(train_dataset[3][0].shape)\n",
    "plot_sequence_form_dataset(train_dataset, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf2e37",
   "metadata": {
    "papermill": {
     "duration": 0.049683,
     "end_time": "2024-03-18T13:21:46.769625",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.719942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31ec59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:46.871572Z",
     "iopub.status.busy": "2024-03-18T13:21:46.870902Z",
     "iopub.status.idle": "2024-03-18T13:21:46.881917Z",
     "shell.execute_reply": "2024-03-18T13:21:46.881046Z"
    },
    "papermill": {
     "duration": 0.064385,
     "end_time": "2024-03-18T13:21:46.884020",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.819635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_divisible(\n",
    "        value: float,\n",
    "        divisor: int,\n",
    "        min_value: Optional[float] = None,\n",
    "        round_down_protect: bool = True,\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    This function is copied from here \n",
    "    \"https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_layers.py\"\n",
    "    \n",
    "    This is to ensure that all layers have channels that are divisible by 8.\n",
    "\n",
    "    Args:\n",
    "        value: A `float` of original value.\n",
    "        divisor: An `int` of the divisor that need to be checked upon.\n",
    "        min_value: A `float` of  minimum value threshold.\n",
    "        round_down_protect: A `bool` indicating whether round down more than 10%\n",
    "        will be allowed.\n",
    "\n",
    "    Returns:\n",
    "        The adjusted value in `int` that is divisible against divisor.\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if round_down_protect and new_value < 0.9 * value:\n",
    "        new_value += divisor\n",
    "    return int(new_value)\n",
    "def conv_2d(inp, oup, kernel_size=3, stride=1, groups=1, bias=False, norm=True, act=True):\n",
    "    conv = nn.Sequential()\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    conv.add_module('conv', nn.Conv3d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
    "    if norm:\n",
    "        conv.add_module('BatchNorm2d', nn.BatchNorm3d(oup))\n",
    "    if act:\n",
    "        conv.add_module('Activation', nn.ReLU6())\n",
    "    return conv\n",
    "\n",
    "\n",
    "# def conv_2d(inp, oup, kernel_size=3, stride=1, groups=1, bias=False, norm=True, act=True):\n",
    "#     conv = nn.Sequential()\n",
    "#     padding = (kernel_size - 1) // 2\n",
    "#     conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
    "#     if norm:\n",
    "#         conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
    "#     if act:\n",
    "#         conv.add_module('Activation', nn.ReLU6())\n",
    "#     return conv\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, act=False, squeeze_excitation=False):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.block = nn.Sequential()\n",
    "        if expand_ratio != 1:\n",
    "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=3, stride=stride))\n",
    "        if squeeze_excitation:\n",
    "            self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, groups=hidden_dim))\n",
    "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, act=act))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "class UniversalInvertedBottleneckBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "            inp, \n",
    "            oup, \n",
    "            start_dw_kernel_size, \n",
    "            middle_dw_kernel_size, \n",
    "            middle_dw_downsample,\n",
    "            stride,\n",
    "            expand_ratio\n",
    "        ):\n",
    "        \"\"\"An inverted bottleneck block with optional depthwises.\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Starting depthwise conv.\n",
    "        self.start_dw_kernel_size = start_dw_kernel_size\n",
    "        if self.start_dw_kernel_size:            \n",
    "            stride_ = stride if not middle_dw_downsample else 1\n",
    "            self._start_dw_ = conv_2d(inp, inp, kernel_size=start_dw_kernel_size, stride=stride_, groups=inp, act=False)\n",
    "        # Expansion with 1x1 convs.\n",
    "        expand_filters = make_divisible(inp * expand_ratio, 8)\n",
    "        self._expand_conv = conv_2d(inp, expand_filters, kernel_size=1)\n",
    "        # Middle depthwise conv.\n",
    "        self.middle_dw_kernel_size = middle_dw_kernel_size\n",
    "        if self.middle_dw_kernel_size:\n",
    "            stride_ = stride if middle_dw_downsample else 1\n",
    "            self._middle_dw = conv_2d(expand_filters, expand_filters, kernel_size=middle_dw_kernel_size, stride=stride_, groups=expand_filters)\n",
    "        # Projection with 1x1 convs.\n",
    "        self._proj_conv = conv_2d(expand_filters, oup, kernel_size=1, stride=1, act=False)\n",
    "        \n",
    "        # Ending depthwise conv.\n",
    "        # this not used\n",
    "        # _end_dw_kernel_size = 0\n",
    "        # self._end_dw = conv_2d(oup, oup, kernel_size=_end_dw_kernel_size, stride=stride, groups=inp, act=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.start_dw_kernel_size:\n",
    "            x = self._start_dw_(x)\n",
    "            # print(\"_start_dw_\", x.shape)\n",
    "        x = self._expand_conv(x)\n",
    "        # print(\"_expand_conv\", x.shape)\n",
    "        if self.middle_dw_kernel_size:\n",
    "            x = self._middle_dw(x)\n",
    "            # print(\"_middle_dw\", x.shape)\n",
    "        x = self._proj_conv(x)\n",
    "        # print(\"_proj_conv\", x.shape)\n",
    "        return x\n",
    "\n",
    "class MultiQueryAttentionLayerWithDownSampling(nn.Module):\n",
    "    def __init__(self, inp, num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides, dw_kernel_size=3, dropout=0.0):\n",
    "        \"\"\"Multi Query Attention with spatial downsampling.\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "\n",
    "        3 parameters are introduced for the spatial downsampling:\n",
    "        1. kv_strides: downsampling factor on Key and Values only.\n",
    "        2. query_h_strides: vertical strides on Query only.\n",
    "        3. query_w_strides: horizontal strides on Query only.\n",
    "\n",
    "        This is an optimized version.\n",
    "        1. Projections in Attention is explict written out as 1x1 Conv2D.\n",
    "        2. Additional reshapes are introduced to bring a up to 3x speed up.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.query_h_strides = query_h_strides\n",
    "        self.query_w_strides = query_w_strides\n",
    "        self.kv_strides = kv_strides\n",
    "        self.dw_kernel_size = dw_kernel_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = key_dim // num_heads\n",
    "\n",
    "        if self.query_h_strides > 1 or self.query_w_strides > 1:\n",
    "            self._query_downsampling_norm = nn.BatchNorm2d(inp)\n",
    "        self._query_proj = conv_2d(inp, num_heads*key_dim, 1, 1, norm=False, act=False)\n",
    "        \n",
    "        if self.kv_strides > 1:\n",
    "            self._key_dw_conv = conv_2d(inp, inp, dw_kernel_size, kv_strides, groups=inp, norm=True, act=False)\n",
    "            self._value_dw_conv = conv_2d(inp, inp, dw_kernel_size, kv_strides, groups=inp, norm=True, act=False)\n",
    "        self._key_proj = conv_2d(inp, key_dim, 1, 1, norm=False, act=False)\n",
    "        self._value_proj = conv_2d(inp, key_dim, 1, 1, norm=False, act=False)\n",
    "\n",
    "        self._output_proj = conv_2d(num_heads*key_dim, inp, 1, 1, norm=False, act=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _, _ = x.size()\n",
    "        if self.query_h_strides > 1 or self.query_w_strides > 1:\n",
    "            q = F.avg_pool2d(self.query_h_stride, self.query_w_stride)\n",
    "            q = self._query_downsampling_norm(q)\n",
    "            q = self._query_proj(q)\n",
    "        else:\n",
    "            q = self._query_proj(x)\n",
    "        px = q.size(2)\n",
    "        q = q.view(batch_size, self.num_heads, -1, self.key_dim) # [batch_size, num_heads, seq_length, key_dim]\n",
    "\n",
    "        if self.kv_strides > 1:\n",
    "            k = self._key_dw_conv(x)\n",
    "            k = self._key_proj(k)\n",
    "            v = self._value_dw_conv(x)\n",
    "            v = self._value_proj(v)          \n",
    "        else:\n",
    "            k = self._key_proj(x)\n",
    "            v = self._value_proj(x)\n",
    "        k = k.view(batch_size, 1, self.key_dim, -1) # [batch_size, 1, key_dim, seq_length]\n",
    "        v = v.view(batch_size, 1, -1, self.key_dim) # [batch_size, 1, seq_length, key_dim]\n",
    "\n",
    "        # calculate attn score\n",
    "        attn_score = torch.matmul(q, k) / (self.head_dim ** 0.5)\n",
    "        attn_score = self.dropout(attn_score)\n",
    "        attn_score = F.softmax(attn_score, dim=-1)\n",
    "\n",
    "        context = torch.matmul(attn_score, v)\n",
    "        context = context.view(batch_size, self.num_heads * self.key_dim, px, px)\n",
    "        output = self._output_proj(context)\n",
    "        return output\n",
    "\n",
    "class MNV4LayerScale(nn.Module):\n",
    "    def __init__(self, inp, init_value):\n",
    "        \"\"\"LayerScale as introduced in CaiT: https://arxiv.org/abs/2103.17239\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "        \n",
    "        As used in MobileNetV4.\n",
    "\n",
    "        Attributes:\n",
    "            init_value (float): value to initialize the diagonal matrix of LayerScale.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.init_value = init_value\n",
    "        self._gamma = nn.Parameter(self.init_value * torch.ones(inp, 1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self._gamma\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            inp,\n",
    "            num_heads, \n",
    "            key_dim,  \n",
    "            value_dim, \n",
    "            query_h_strides, \n",
    "            query_w_strides, \n",
    "            kv_strides,\n",
    "            use_layer_scale,\n",
    "            use_multi_query, \n",
    "            use_residual = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.query_h_strides = query_h_strides\n",
    "        self.query_w_strides = query_w_strides\n",
    "        self.kv_strides = kv_strides\n",
    "        self.use_layer_scale = use_layer_scale\n",
    "        self.use_multi_query = use_multi_query\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        self._input_norm = nn.BatchNorm2d(inp)\n",
    "        if self.use_multi_query:\n",
    "            self.multi_query_attention = MultiQueryAttentionLayerWithDownSampling(\n",
    "                inp, num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides\n",
    "            )\n",
    "        else:\n",
    "            self.multi_head_attention = nn.MultiheadAttention(inp, num_heads, kdim=key_dim)\n",
    "        \n",
    "        if self.use_layer_scale:\n",
    "            self.layer_scale_init_value = 1e-5\n",
    "            self.layer_scale = MNV4LayerScale(inp, self.layer_scale_init_value) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Not using CPE, skipped\n",
    "        # input norm\n",
    "        shortcut = x\n",
    "        x = self._input_norm(x)\n",
    "        # multi query\n",
    "        if self.use_multi_query:\n",
    "            x = self.multi_query_attention(x)\n",
    "        else:\n",
    "            x = self.multi_head_attention(x, x)\n",
    "        # layer scale\n",
    "        if self.use_layer_scale:\n",
    "            x = self.layer_scale(x)\n",
    "        # use residual\n",
    "        if self.use_residual:\n",
    "            x = x + shortcut\n",
    "        return x\n",
    "\n",
    "def build_blocks(layer_spec):\n",
    "    if not layer_spec.get('block_name'):\n",
    "        return nn.Sequential()\n",
    "    block_names = layer_spec['block_name']\n",
    "    layers = nn.Sequential()\n",
    "    if block_names == \"convbn\":\n",
    "        schema_ = ['inp', 'oup', 'kernel_size', 'stride']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            layers.add_module(f\"convbn_{i}\", conv_2d(**args))\n",
    "    elif block_names == \"uib\":\n",
    "        schema_ =  ['inp', 'oup', 'start_dw_kernel_size', 'middle_dw_kernel_size', 'middle_dw_downsample', 'stride', 'expand_ratio', 'mhsa']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            mhsa = args.pop(\"mhsa\") if \"mhsa\" in args else 0\n",
    "            layers.add_module(f\"uib_{i}\", UniversalInvertedBottleneckBlock(**args))\n",
    "            if mhsa:\n",
    "                mhsa_schema_ = [\n",
    "                    \"inp\", \"num_heads\", \"key_dim\", \"value_dim\", \"query_h_strides\", \"query_w_strides\", \"kv_strides\", \n",
    "                    \"use_layer_scale\", \"use_multi_query\", \"use_residual\"\n",
    "                ]\n",
    "                args = dict(zip(mhsa_schema_, [args['oup']] + (mhsa)))\n",
    "                layers.add_module(f\"mhsa_{i}\", MultiHeadSelfAttentionBlock(**args))\n",
    "    elif block_names == \"fused_ib\":\n",
    "        schema_ = ['inp', 'oup', 'stride', 'expand_ratio', 'act']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            layers.add_module(f\"fused_ib_{i}\", InvertedResidual(**args))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return layers\n",
    "\n",
    "\n",
    "class MobileNetV4(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        # MobileNetV4ConvSmall  MobileNetV4ConvMedium  MobileNetV4ConvLarge\n",
    "        # MobileNetV4HybridMedium  MobileNetV4HybridLarge\n",
    "        \"\"\"Params to initiate MobilenNetV4\n",
    "        Args:\n",
    "            model : support 5 types of models as indicated in \n",
    "            \"https://github.com/tensorflow/models/blob/master/official/vision/modeling/backbones/mobilenet.py\"        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert model in MODEL_SPECS.keys()\n",
    "        self.model = model\n",
    "        self.spec = MODEL_SPECS[self.model]\n",
    "       \n",
    "        # conv0\n",
    "        self.conv0 = build_blocks(self.spec['conv0'])\n",
    "        # layer1\n",
    "        self.layer1 = build_blocks(self.spec['layer1'])\n",
    "        # layer2\n",
    "        self.layer2 = build_blocks(self.spec['layer2'])\n",
    "        # layer3\n",
    "        self.layer3 = build_blocks(self.spec['layer3'])\n",
    "        # layer4\n",
    "        self.layer4 = build_blocks(self.spec['layer4'])\n",
    "        # layer5   \n",
    "        self.layer5 = build_blocks(self.spec['layer5'])       \n",
    "               \n",
    "    def forward(self, x):\n",
    "        x0 = self.conv0(x)\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        x5 = nn.functional.adaptive_avg_pool2d(x5, 1 )\n",
    "        return [x1, x2, x3, x4, x5]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MNV4ConvSmall_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [32, 32, 3, 2],\n",
    "            [32, 32, 1, 1]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [32, 96, 3, 2],\n",
    "            [96, 64, 1, 1]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 6,\n",
    "        \"block_specs\": [\n",
    "            [64, 96, 5, 5, True, 2, 3],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 3, 0, True, 1, 4],\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 6,\n",
    "        \"block_specs\": [\n",
    "            [96,  128, 3, 3, True, 2, 6],\n",
    "            [128, 128, 5, 5, True, 1, 4],\n",
    "            [128, 128, 0, 5, True, 1, 4],\n",
    "            [128, 128, 0, 5, True, 1, 3],\n",
    "            [128, 128, 0, 3, True, 1, 4],\n",
    "            [128, 128, 0, 3, True, 1, 4],\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [128, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4ConvMedium_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [32, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 80, 3, 5, True, 2, 4],\n",
    "            [80, 80, 3, 3, True, 1, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 8,\n",
    "        \"block_specs\": [\n",
    "            [80,  160, 3, 5, True, 2, 6],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 5, True, 1, 4],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 0, True, 1, 4],\n",
    "            [160, 160, 0, 0, True, 1, 2],\n",
    "            [160, 160, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [160, 256, 5, 5, True, 2, 6],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 3, 0, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 2],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 5, 0, True, 1, 2]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [256, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4ConvLarge_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 24, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [24, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 96, 3, 5, True, 2, 4],\n",
    "            [96, 96, 3, 3, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [96,  192, 3, 5, True, 2, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 5, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 13,\n",
    "        \"block_specs\": [\n",
    "            [192, 512, 5, 5, True, 2, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [512, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def mhsa(num_heads, key_dim, value_dim, px):\n",
    "    if px == 24:\n",
    "        kv_strides = 2\n",
    "    elif px == 12:\n",
    "        kv_strides = 1\n",
    "    query_h_strides = 1 \n",
    "    query_w_strides = 1 \n",
    "    use_layer_scale = True \n",
    "    use_multi_query = True\n",
    "    use_residual = True\n",
    "    return [\n",
    "        num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides, \n",
    "        use_layer_scale, use_multi_query, use_residual\n",
    "    ]\n",
    "\n",
    "MNV4HybridConvMedium_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [3, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [32, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 80, 3, 5, True, 2, 4],\n",
    "            [80, 80, 3, 3, True, 1, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 8,\n",
    "        \"block_specs\": [\n",
    "            [80,  160, 3, 5, True, 2, 6],\n",
    "            [160, 160, 0, 0, True, 1, 2],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 5, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 3, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 0, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 3, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 12,\n",
    "        \"block_specs\": [\n",
    "            [160, 256, 5, 5, True, 2, 6],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 2],\n",
    "            [256, 256, 3, 5, True, 1, 2],\n",
    "            [256, 256, 0, 0, True, 1, 2],\n",
    "            [256, 256, 0, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 3, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 5, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [256, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4HybridConvLarge_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [3, 24, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [24, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 96, 3, 5, True, 2, 4],\n",
    "            [96, 96, 3, 3, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [96,  192, 3, 5, True, 2, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 5, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 14,\n",
    "        \"block_specs\": [\n",
    "            [192, 512, 5, 5, True, 2, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [512, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    \"MobileNetV4ConvSmall\": MNV4ConvSmall_BLOCK_SPECS,\n",
    "    \"MobileNetV4ConvMedium\": MNV4ConvMedium_BLOCK_SPECS,\n",
    "    \"MobileNetV4ConvLarge\": MNV4ConvLarge_BLOCK_SPECS,\n",
    "    \"MobileNetV4HybridMedium\": MNV4HybridConvMedium_BLOCK_SPECS,\n",
    "    \"MobileNetV4HybridLarge\": MNV4HybridConvLarge_BLOCK_SPECS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d75303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Support ['MobileNetV4ConvSmall', 'MobileNetV4ConvMedium', 'MobileNetV4ConvLarge']\n",
    "# # Also supported ['MobileNetV4HybridMedium', 'MobileNetV4HybridLarge']\n",
    "# model = MobileNetV4(\"MobileNetV4ConvSmall\")\n",
    "\n",
    "# # Check the trainable params\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "# # Check the model's output shape\n",
    "# print(\"Check output shape ...\")\n",
    "# x = torch.rand(4, 1, 8, 64, 64)\n",
    "# y = model(x)\n",
    "# for i in y:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 모델 정의\n",
    "class FCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.fc = nn.Linear(1280, 14)  # 예시로 출력 뉴런 수를 10개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn = MobileNetV4(\"MobileNetV4ConvSmall\")\n",
    "        self.fc = FCModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x[4]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedModel()\n",
    "x = torch.rand(4, 1, 8, 64, 64)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc3dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8279d20",
   "metadata": {
    "papermill": {
     "duration": 0.058478,
     "end_time": "2024-03-18T13:21:48.387320",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.328842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa6ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:48.502470Z",
     "iopub.status.busy": "2024-03-18T13:21:48.501607Z",
     "iopub.status.idle": "2024-03-18T13:21:48.506944Z",
     "shell.execute_reply": "2024-03-18T13:21:48.506075Z"
    },
    "papermill": {
     "duration": 0.064273,
     "end_time": "2024-03-18T13:21:48.508993",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.444720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03347ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:48.613921Z",
     "iopub.status.busy": "2024-03-18T13:21:48.613620Z",
     "iopub.status.idle": "2024-03-18T13:21:48.993707Z",
     "shell.execute_reply": "2024-03-18T13:21:48.992772Z"
    },
    "papermill": {
     "duration": 0.433887,
     "end_time": "2024-03-18T13:21:48.995895",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.562008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed03c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.103520Z",
     "iopub.status.busy": "2024-03-18T13:21:49.103183Z",
     "iopub.status.idle": "2024-03-18T13:21:49.107298Z",
     "shell.execute_reply": "2024-03-18T13:21:49.106444Z"
    },
    "papermill": {
     "duration": 0.057751,
     "end_time": "2024-03-18T13:21:49.109369",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.051618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b489c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.211609Z",
     "iopub.status.busy": "2024-03-18T13:21:49.210820Z",
     "iopub.status.idle": "2024-03-18T13:21:49.215238Z",
     "shell.execute_reply": "2024-03-18T13:21:49.214354Z"
    },
    "papermill": {
     "duration": 0.058531,
     "end_time": "2024-03-18T13:21:49.217169",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.158638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8597c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.320218Z",
     "iopub.status.busy": "2024-03-18T13:21:49.319329Z",
     "iopub.status.idle": "2024-03-18T13:24:03.470004Z",
     "shell.execute_reply": "2024-03-18T13:24:03.468897Z"
    },
    "papermill": {
     "duration": 134.204958,
     "end_time": "2024-03-18T13:24:03.472064",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.267106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_batches = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    _total_correct = 0\n",
    "    \n",
    "    print(f\"Epoch : {epoch}...\")\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        print(f\"Batch : {batch_idx + 1}/{total_batches}\\t\\t\", end=\"\\r\")\n",
    "        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        preds = model(data)\n",
    "        \n",
    "        # print(preds.shape, label.shape)\n",
    "        # print(label)\n",
    "        \n",
    "        loss = criterion(preds, label.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx == 4290:\n",
    "            break\n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"Loss: {loss.item()}, Batch: {batch_idx + 1}/{total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a098b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:24:03.667669Z",
     "iopub.status.busy": "2024-03-18T13:24:03.667298Z",
     "iopub.status.idle": "2024-03-18T13:24:03.708897Z",
     "shell.execute_reply": "2024-03-18T13:24:03.707912Z"
    },
    "papermill": {
     "duration": 0.140467,
     "end_time": "2024-03-18T13:24:03.711313",
     "exception": false,
     "start_time": "2024-03-18T13:24:03.570846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"Mobilenetv4_convs_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffba4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:24:03.920564Z",
     "iopub.status.busy": "2024-03-18T13:24:03.920239Z",
     "iopub.status.idle": "2024-03-18T13:24:05.640962Z",
     "shell.execute_reply": "2024-03-18T13:24:05.640021Z"
    },
    "papermill": {
     "duration": 1.833851,
     "end_time": "2024-03-18T13:24:05.646746",
     "exception": false,
     "start_time": "2024-03-18T13:24:03.812895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_randomly_form_dataset_model(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78452d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"20240716_C3d+LSTM_model.pt\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(labels)):\n",
    "                if (predicted[i] == labels[i]):\n",
    "                    correct += 1\n",
    "            \n",
    "           \n",
    "            print(labels.squeeze())\n",
    "            print(\"predicted = {},\\n total  = {}, correct = {}\".format(predicted,total,correct))\n",
    "            \n",
    "            # print(outputs)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f'Accuracy of the model on the test dataset: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ad83a",
   "metadata": {},
   "source": [
    "# For New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141148dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, __train=True, __train_test_split=1, __random_state=42):\n",
    "        torch.manual_seed(__random_state)\n",
    "        \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        self._inclusion_probability = __train_test_split if __train else 1.0 - __train_test_split\n",
    "        self._frame_interval = frame_interval\n",
    "        \n",
    "        print(f\"Loading {'train' if __train else 'test' } dataset...\")\n",
    "        for label, data_path in TEST_SOURCE.items():\n",
    "            print(f\"Loading Label {label}...\")\n",
    "            for file in tqdm(os.listdir(data_path)):\n",
    "                if file.endswith(\".mp4\"):\n",
    "                    path = os.path.join(data_path, file)\n",
    "                    data, labels = self._parse_file(path, label)\n",
    "                    # print(data)\n",
    "                    # print(labels)\n",
    "                    # print(data[0].shape)\n",
    "                    # print(len(data))\n",
    "                    # print(len(labels))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    self._data.extend(data)\n",
    "                    self._labels.extend(labels)\n",
    "        print(f\"Finished loading {'train' if __train else 'test' } dataset... Loaded  {len(self._data)} images.\")\n",
    "    \n",
    "    def _parse_file(self, __path, __label):\n",
    "        if not os.path.exists(__path):\n",
    "            return [], []\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        cap = cv2.VideoCapture(__path)\n",
    "        \n",
    "        frames = []\n",
    "        success, image = cap.read()\n",
    "        while success:\n",
    "            try:\n",
    "                if True:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "                    Y, U, V = cv2.split(image)\n",
    "                    image = transformer(Y)\n",
    "                    frames.append(image)\n",
    "                    \n",
    "                    if len(frames) == block_frame:\n",
    "                        stacked_frames = torch.stack(frames)\n",
    "                        stacked_frames = stacked_frames.permute(1, 0, 2, 3)\n",
    "                        data.append(stacked_frames)\n",
    "                        labels.append(__label)\n",
    "                        frames = frames[3:]\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {__path}: {e}\")\n",
    "            \n",
    "            count = 0\n",
    "            while success and count < self._frame_interval:\n",
    "                success, image = cap.read()\n",
    "                count += 1\n",
    "                \n",
    "        if len(frames) > 0 and len(frames) < block_frame:\n",
    "            while len(frames) < block_frame:\n",
    "                frames.append(frames[-1])\n",
    "            stacked_frames = torch.stack(frames)\n",
    "            stacked_frames = stacked_frames.permute(1, 0, 2, 3)\n",
    "            data.append(stacked_frames)\n",
    "            labels.append(__label)        \n",
    "                \n",
    "        return data, labels\n",
    "        \n",
    "    \n",
    "    def label_str2id(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels).index(__label)\n",
    "\n",
    "    def label_id2str(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels)[__label]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, __idx):\n",
    "        data = self._data[__idx]\n",
    "        label = self._labels[__idx]\n",
    "        return data, torch.tensor([self.label_str2id(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = testDataset(True)\n",
    "plot_randomly_form_dataset(newtest)\n",
    "newtest_loader = DataLoader(newtest, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_randomly_form_dataset_model(newtest, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"20240716_C3d+LSTM_model.pt\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(labels)):\n",
    "                if (predicted[i] == labels[i]):\n",
    "                    correct += 1\n",
    "            \n",
    "           \n",
    "            print(labels.squeeze())\n",
    "            print(\"predicted = {},\\n total  = {}, correct = {}\".format(predicted,total,correct))\n",
    "            \n",
    "            # print(outputs)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = calculate_accuracy(model, newtest_loader, device)\n",
    "print(f'Accuracy of the model on the test dataset: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592101a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 75548,
     "sourceId": 170620,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 824.711442,
   "end_time": "2024-03-18T13:24:09.430326",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T13:10:24.718884",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
