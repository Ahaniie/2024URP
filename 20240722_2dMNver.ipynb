{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc08077",
   "metadata": {
    "papermill": {
     "duration": 0.008184,
     "end_time": "2024-03-18T13:10:42.279133",
     "exception": false,
     "start_time": "2024-03-18T13:10:42.270949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Stuff & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80caa367",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:42.297330Z",
     "iopub.status.busy": "2024-03-18T13:10:42.297029Z",
     "iopub.status.idle": "2024-03-18T13:10:50.995995Z",
     "shell.execute_reply": "2024-03-18T13:10:50.994940Z"
    },
    "papermill": {
     "duration": 8.710986,
     "end_time": "2024-03-18T13:10:50.998486",
     "exception": false,
     "start_time": "2024-03-18T13:10:42.287500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3bd6dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.017736Z",
     "iopub.status.busy": "2024-03-18T13:10:51.017143Z",
     "iopub.status.idle": "2024-03-18T13:10:51.107888Z",
     "shell.execute_reply": "2024-03-18T13:10:51.106932Z"
    },
    "papermill": {
     "duration": 0.102383,
     "end_time": "2024-03-18T13:10:51.109927",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.007544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904f3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c122a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.129159Z",
     "iopub.status.busy": "2024-03-18T13:10:51.128887Z",
     "iopub.status.idle": "2024-03-18T13:10:51.132703Z",
     "shell.execute_reply": "2024-03-18T13:10:51.131901Z"
    },
    "papermill": {
     "duration": 0.015454,
     "end_time": "2024-03-18T13:10:51.134564",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.119110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 64\n",
    "block_frame = 9\n",
    "frame_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2c4ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.152594Z",
     "iopub.status.busy": "2024-03-18T13:10:51.152320Z",
     "iopub.status.idle": "2024-03-18T13:10:51.156215Z",
     "shell.execute_reply": "2024-03-18T13:10:51.155441Z"
    },
    "papermill": {
     "duration": 0.014977,
     "end_time": "2024-03-18T13:10:51.158055",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.143078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d1d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.177027Z",
     "iopub.status.busy": "2024-03-18T13:10:51.176780Z",
     "iopub.status.idle": "2024-03-18T13:10:51.181166Z",
     "shell.execute_reply": "2024-03-18T13:10:51.180357Z"
    },
    "papermill": {
     "duration": 0.015692,
     "end_time": "2024-03-18T13:10:51.183090",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.167398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # transforms.Lambda(lambda x: x / 255.),\n",
    "    transforms.Resize((RESOLUTION, RESOLUTION))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c986f9",
   "metadata": {
    "papermill": {
     "duration": 0.008199,
     "end_time": "2024-03-18T13:10:51.199759",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.191560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4673b989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.218056Z",
     "iopub.status.busy": "2024-03-18T13:10:51.217793Z",
     "iopub.status.idle": "2024-03-18T13:10:51.224015Z",
     "shell.execute_reply": "2024-03-18T13:10:51.223123Z"
    },
    "papermill": {
     "duration": 0.01771,
     "end_time": "2024-03-18T13:10:51.225987",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.208277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_image_from_list(__images, __labels, __count):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for _i in range(__count[0] * __count[1]):\n",
    "        plt.subplot(__count[0], __count[1], _i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        _img = __images[_i].to(\"cpu\").numpy()\n",
    "        plt.imshow(_img, cmap= \"gray\")\n",
    "        plt.xlabel(__labels[_i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74a577b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.244593Z",
     "iopub.status.busy": "2024-03-18T13:10:51.244285Z",
     "iopub.status.idle": "2024-03-18T13:10:51.249956Z",
     "shell.execute_reply": "2024-03-18T13:10:51.249074Z"
    },
    "papermill": {
     "duration": 0.017468,
     "end_time": "2024-03-18T13:10:51.251954",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.234486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_randomly_form_dataset(__dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _ in range(0, 25):\n",
    "        index = torch.randint(0, len(__dataset), (1,)).item()\n",
    "        image, label = __dataset[index]\n",
    "        images.append(image.squeeze())\n",
    "        labels.append(__dataset.label_id2str(label.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (5, 5))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5adc1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_randomly_form_dataset(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39m_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "plot_randomly_form_dataset(train_dataset)\n",
    "print(train_dataset._labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf33a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequence_form_dataset(__dataset, index):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(0, block_frame):\n",
    "        image, label = __dataset[index]\n",
    "        images.append(image.squeeze())\n",
    "        labels.append(__dataset.label_id2str(label.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (1, block_frame))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da33dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_randomly_form_dataset_model(__dataset, __model):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _ in range(0, 25):\n",
    "        index = torch.randint(0, len(__dataset), (1,)).item()\n",
    "        image, label = __dataset[index]\n",
    "        images.append(image.squeeze())\n",
    "        result = model(image).squeeze().cpu().argmax()\n",
    "        \n",
    "        labels.append(__dataset.label_id2str(label.item()) + \" -> \" + __dataset.label_id2str(result.item()))\n",
    "    \n",
    "    plot_image_from_list(images, labels, (5, 5))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ae748",
   "metadata": {
    "papermill": {
     "duration": 0.008208,
     "end_time": "2024-03-18T13:10:51.296122",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.287914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75883e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.314167Z",
     "iopub.status.busy": "2024-03-18T13:10:51.313885Z",
     "iopub.status.idle": "2024-03-18T13:10:51.318124Z",
     "shell.execute_reply": "2024-03-18T13:10:51.317353Z"
    },
    "papermill": {
     "duration": 0.015519,
     "end_time": "2024-03-18T13:10:51.320057",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.304538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE = {\n",
    "    \"Abuse\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-1\\Anomaly-Videos-Part-1\\Abuse\",\n",
    "    \"Arrest\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-1\\Anomaly-Videos-Part-1\\Arrest\",\n",
    "    \"Arson\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-1\\Anomaly-Videos-Part-1\\Arson\",\n",
    "    \"Assault\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-1\\Anomaly-Videos-Part-1\\Assault\",\n",
    "    \"Burglary\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-2\\Anomaly-Videos-Part-2\\Burglary\",\n",
    "    \"Explosion\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-2\\Anomaly-Videos-Part-2\\Explosion\",\n",
    "    \"Fighting\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-2\\Anomaly-Videos-Part-2\\Fighting\",   \n",
    "    \"RoadAccidents\":r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-3\\Anomaly-Videos-Part-3\\RoadAccidents\",\n",
    "    \"Robbery\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-3\\Anomaly-Videos-Part-3\\Robbery\",\n",
    "    \"Shooting\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-3\\Anomaly-Videos-Part-3\\Shooting\",\n",
    "    \"Shoplifting\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-4\\Anomaly-Videos-Part-4\\Shoplifting\",\n",
    "    \"Stealing\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-4\\Anomaly-Videos-Part-4\\Stealing\",\n",
    "    \"Vandalism\": r\"D:\\Anomaly-Detection-Dataset\\Anomaly-Videos-Part-4\\Anomaly-Videos-Part-4\\Vandalism\",\n",
    "    \"Normal\": r\"D:\\Anomaly-Detection-Dataset\\Training-Normal-Videos-Part-1\\Training-Normal-Videos-Part-1\"\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA_SOURCE = {\n",
    "#     \"Abuse\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Abuse\",\n",
    "#     \"Arrest\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Arrest\",\n",
    "#     \"Arson\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Arson\",\n",
    "#     \"Assault\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Assault\",\n",
    "#     \"Burglary\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Burglary\",\n",
    "#     \"Explosion\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Explosion\",\n",
    "#     \"Fighting\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Fighting\",   \n",
    "#     \"RoadAccidents\":\"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\RoadAccidents\",\n",
    "#     \"Robbery\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Robbery\",\n",
    "#     \"Shooting\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Shooting\",\n",
    "#     \"Shoplifting\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Shoplifting\",\n",
    "#     \"Stealing\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Stealing\",\n",
    "#     \"Vandalism\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Vandalism\",\n",
    "#     \"Normal\": \"D:\\\\Anomaly-Detection-Dataset\\\\Miniset_for_train_1\\\\Normal\"\n",
    "# }\n",
    "\n",
    "\n",
    "# DATA_SOURCE = {\n",
    "#        \"Abuse\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Abuse\",\n",
    "#     \"Arrest\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Arrest\",\n",
    "#     \"Arson\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Arson\",\n",
    "#     \"Assault\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-1/Anomaly-Videos-Part-1/Assault\",\n",
    "#     \"Burglary\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Burglary\",\n",
    "#     \"Explosion\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Explosion\",\n",
    "#     \"Fighting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-2/Anomaly-Videos-Part-2/Fighting\",   \n",
    "#     \"RoadAccidents\":\"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/RoadAccidents\",\n",
    "#     \"Robbery\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/Robbery\",\n",
    "#     \"Shooting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-3/Anomaly-Videos-Part-3/Shooting\",\n",
    "#     \"Shoplifting\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Shoplifting\",\n",
    "#     \"Stealing\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Stealing\",\n",
    "#     \"Vandalism\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Anomaly-Videos-Part-4/Anomaly-Videos-Part-4/Vandalism\",\n",
    "#     \"Normal\": \"/home/yhwoo7/Anomaly-Detection-Dataset/Training-Normal-Videos-Part-1/Training-Normal-Videos-Part-1\"\n",
    "# }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85661581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "txt_train = \"D:\\Anomaly-Detection-Dataset\\supermini_train.txt\"\n",
    "train_dic = {}\n",
    "with open(txt_train, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('/')\n",
    "        # print(parts)\n",
    "        train_dic[parts[1]] = parts[0]\n",
    "    \n",
    "txt_test = \"D:\\Anomaly-Detection-Dataset\\supermini_test.txt\"\n",
    "test_dic = {}\n",
    "with open(txt_test, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        video_name = parts[0]\n",
    "        event_name = parts[1]\n",
    "        start_frame1 = int(parts[2])\n",
    "        end_frame1 = int(parts[3])\n",
    "        start_frame2 = int(parts[4])\n",
    "        end_frame2 = int(parts[5])\n",
    "        test_dic[video_name] = [event_name, start_frame1, end_frame1, start_frame2, end_frame2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7562159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Abuse001_x264.mp4', 'Abuse002_x264.mp4', 'Abuse003_x264.mp4', 'Abuse004_x264.mp4', 'Abuse005_x264.mp4', 'Abuse006_x264.mp4', 'Abuse007_x264.mp4', 'Abuse008_x264.mp4', 'Abuse009_x264.mp4', 'Abuse010_x264.mp4', 'Abuse011_x264.mp4', 'Abuse012_x264.mp4', 'Abuse013_x264.mp4', 'Abuse014_x264.mp4', 'Abuse015_x264.mp4', 'Abuse016_x264.mp4', 'Abuse017_x264.mp4', 'Abuse018_x264.mp4', 'Abuse019_x264.mp4', 'Abuse020_x264.mp4', 'Abuse021_x264.mp4', 'Abuse022_x264.mp4', 'Abuse023_x264.mp4', 'Abuse024_x264.mp4', 'Abuse025_x264.mp4', 'Abuse026_x264.mp4', 'Abuse027_x264.mp4', 'Abuse028_x264.mp4', 'Abuse029_x264.mp4', 'Abuse030_x264.mp4', 'Abuse031_x264.mp4', 'Abuse032_x264.mp4', 'Abuse033_x264.mp4', 'Abuse034_x264.mp4', 'Abuse035_x264.mp4', 'Abuse036_x264.mp4', 'Abuse037_x264.mp4', 'Abuse038_x264.mp4', 'Abuse039_x264.mp4', 'Abuse040_x264.mp4', 'Abuse041_x264.mp4', 'Abuse042_x264.mp4', 'Abuse043_x264.mp4', 'Abuse044_x264.mp4', 'Abuse045_x264.mp4', 'Abuse046_x264.mp4', 'Abuse047_x264.mp4', 'Abuse048_x264.mp4', 'Abuse049_x264.mp4', 'Abuse050_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Arrest001_x264.mp4', 'Arrest002_x264.mp4', 'Arrest003_x264.mp4', 'Arrest004_x264.mp4', 'Arrest005_x264.mp4', 'Arrest006_x264.mp4', 'Arrest007_x264.mp4', 'Arrest008_x264.mp4', 'Arrest009_x264.mp4', 'Arrest010_x264.mp4', 'Arrest011_x264.mp4', 'Arrest012_x264.mp4', 'Arrest013_x264.mp4', 'Arrest014_x264.mp4', 'Arrest015_x264.mp4', 'Arrest016_x264.mp4', 'Arrest017_x264.mp4', 'Arrest018_x264.mp4', 'Arrest019_x264.mp4', 'Arrest020_x264.mp4', 'Arrest021_x264.mp4', 'Arrest022_x264.mp4', 'Arrest023_x264.mp4', 'Arrest024_x264.mp4', 'Arrest025_x264.mp4', 'Arrest026_x264.mp4', 'Arrest027_x264.mp4', 'Arrest028_x264.mp4', 'Arrest029_x264.mp4', 'Arrest030_x264.mp4', 'Arrest031_x264.mp4', 'Arrest032_x264.mp4', 'Arrest033_x264.mp4', 'Arrest034_x264.mp4', 'Arrest035_x264.mp4', 'Arrest036_x264.mp4', 'Arrest037_x264.mp4', 'Arrest038_x264.mp4', 'Arrest039_x264.mp4', 'Arrest040_x264.mp4', 'Arrest041_x264.mp4', 'Arrest042_x264.mp4', 'Arrest043_x264.mp4', 'Arrest044_x264.mp4', 'Arrest046_x264.mp4', 'Arrest047_x264.mp4', 'Arrest048_x264.mp4', 'Arrest049_x264.mp4', 'Arrest050_x264.mp4', 'Arrest051_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Arson001_x264.mp4', 'Arson002_x264.mp4', 'Arson003_x264.mp4', 'Arson005_x264.mp4', 'Arson006_x264.mp4', 'Arson007_x264.mp4', 'Arson008_x264.mp4', 'Arson009_x264.mp4', 'Arson010_x264.mp4', 'Arson011_x264.mp4', 'Arson012_x264.mp4', 'Arson013_x264.mp4', 'Arson014_x264.mp4', 'Arson015_x264.mp4', 'Arson016_x264.mp4', 'Arson017_x264.mp4', 'Arson018_x264.mp4', 'Arson019_x264.mp4', 'Arson020_x264.mp4', 'Arson021_x264.mp4', 'Arson022_x264.mp4', 'Arson023_x264.mp4', 'Arson024_x264.mp4', 'Arson025_x264.mp4', 'Arson026_x264.mp4', 'Arson027_x264.mp4', 'Arson028_x264.mp4', 'Arson029_x264.mp4', 'Arson030_x264.mp4', 'Arson031_x264.mp4', 'Arson032_x264.mp4', 'Arson034_x264.mp4', 'Arson035_x264.mp4', 'Arson036_x264.mp4', 'Arson037_x264.mp4', 'Arson038_x264.mp4', 'Arson039_x264.mp4', 'Arson040_x264.mp4', 'Arson041_x264.mp4', 'Arson042_x264.mp4', 'Arson044_x264.mp4', 'Arson045_x264.mp4', 'Arson046_x264.mp4', 'Arson047_x264.mp4', 'Arson048_x264.mp4', 'Arson049_x264.mp4', 'Arson050_x264.mp4', 'Arson051_x264.mp4', 'Arson052_x264.mp4', 'Arson053_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Assault001_x264.mp4', 'Assault002_x264.mp4', 'Assault003_x264.mp4', 'Assault004_x264.mp4', 'Assault005_x264.mp4', 'Assault006_x264.mp4', 'Assault007_x264.mp4', 'Assault008_x264.mp4', 'Assault009_x264.mp4', 'Assault010_x264.mp4', 'Assault011_x264.mp4', 'Assault012_x264.mp4', 'Assault013_x264.mp4', 'Assault014_x264.mp4', 'Assault015_x264.mp4', 'Assault016_x264.mp4', 'Assault017_x264.mp4', 'Assault018_x264.mp4', 'Assault019_x264.mp4', 'Assault020_x264.mp4', 'Assault022_x264.mp4', 'Assault023_x264.mp4', 'Assault024_x264.mp4', 'Assault025_x264.mp4', 'Assault026_x264.mp4', 'Assault027_x264.mp4', 'Assault028_x264.mp4', 'Assault029_x264.mp4', 'Assault030_x264.mp4', 'Assault031_x264.mp4', 'Assault032_x264.mp4', 'Assault033_x264.mp4', 'Assault034_x264.mp4', 'Assault035_x264.mp4', 'Assault036_x264.mp4', 'Assault037_x264.mp4', 'Assault038_x264.mp4', 'Assault039_x264.mp4', 'Assault040_x264.mp4', 'Assault041_x264.mp4', 'Assault042_x264.mp4', 'Assault044_x264.mp4', 'Assault045_x264.mp4', 'Assault046_x264.mp4', 'Assault047_x264.mp4', 'Assault048_x264.mp4', 'Assault049_x264.mp4', 'Assault050_x264.mp4', 'Assault051_x264.mp4', 'Assault052_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Burglary001_x264.mp4', 'Burglary002_x264.mp4', 'Burglary003_x264.mp4', 'Burglary004_x264.mp4', 'Burglary005_x264.mp4', 'Burglary006_x264.mp4', 'Burglary007_x264.mp4', 'Burglary008_x264.mp4', 'Burglary009_x264.mp4', 'Burglary010_x264.mp4', 'Burglary011_x264.mp4', 'Burglary012_x264.mp4', 'Burglary013_x264.mp4', 'Burglary014_x264.mp4', 'Burglary015_x264.mp4', 'Burglary016_x264.mp4', 'Burglary017_x264.mp4', 'Burglary018_x264.mp4', 'Burglary019_x264.mp4', 'Burglary020_x264.mp4', 'Burglary021_x264.mp4', 'Burglary022_x264.mp4', 'Burglary023_x264.mp4', 'Burglary024_x264.mp4', 'Burglary025_x264.mp4', 'Burglary026_x264.mp4', 'Burglary027_x264.mp4', 'Burglary028_x264.mp4', 'Burglary029_x264.mp4', 'Burglary030_x264.mp4', 'Burglary031_x264.mp4', 'Burglary032_x264.mp4', 'Burglary033_x264.mp4', 'Burglary034_x264.mp4', 'Burglary035_x264.mp4', 'Burglary036_x264.mp4', 'Burglary037_x264.mp4', 'Burglary038_x264.mp4', 'Burglary039_x264.mp4', 'Burglary040_x264.mp4', 'Burglary041_x264.mp4', 'Burglary042_x264.mp4', 'Burglary043_x264.mp4', 'Burglary044_x264.mp4', 'Burglary045_x264.mp4', 'Burglary046_x264.mp4', 'Burglary047_x264.mp4', 'Burglary048_x264.mp4', 'Burglary049_x264.mp4', 'Burglary050_x264.mp4', 'Burglary051_x264.mp4', 'Burglary052_x264.mp4', 'Burglary053_x264.mp4', 'Burglary054_x264.mp4', 'Burglary055_x264.mp4', 'Burglary056_x264.mp4', 'Burglary057_x264.mp4', 'Burglary058_x264.mp4', 'Burglary059_x264.mp4', 'Burglary060_x264.mp4', 'Burglary061_x264.mp4', 'Burglary062_x264.mp4', 'Burglary063_x264.mp4', 'Burglary064_x264.mp4', 'Burglary065_x264.mp4', 'Burglary066_x264.mp4', 'Burglary067_x264.mp4', 'Burglary068_x264.mp4', 'Burglary069_x264.mp4', 'Burglary070_x264.mp4', 'Burglary071_x264.mp4', 'Burglary072_x264.mp4', 'Burglary073_x264.mp4', 'Burglary074_x264.mp4', 'Burglary075_x264.mp4', 'Burglary076_x264.mp4', 'Burglary077_x264.mp4', 'Burglary078_x264.mp4', 'Burglary079_x264.mp4', 'Burglary080_x264.mp4', 'Burglary081_x264.mp4', 'Burglary082_x264.mp4', 'Burglary083_x264.mp4', 'Burglary084_x264.mp4', 'Burglary085_x264.mp4', 'Burglary086_x264.mp4', 'Burglary087_x264.mp4', 'Burglary088_x264.mp4', 'Burglary089_x264.mp4', 'Burglary090_x264.mp4', 'Burglary091_x264.mp4', 'Burglary092_x264.mp4', 'Burglary093_x264.mp4', 'Burglary094_x264.mp4', 'Burglary095_x264.mp4', 'Burglary096_x264.mp4', 'Burglary097_x264.mp4', 'Burglary098_x264.mp4', 'Burglary099_x264.mp4', 'Burglary100_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Explosion001_x264.mp4', 'Explosion002_x264.mp4', 'Explosion003_x264.mp4', 'Explosion004_x264.mp4', 'Explosion005_x264.mp4', 'Explosion006_x264.mp4', 'Explosion007_x264.mp4', 'Explosion008_x264.mp4', 'Explosion009_x264.mp4', 'Explosion010_x264.mp4', 'Explosion011_x264.mp4', 'Explosion012_x264.mp4', 'Explosion013_x264.mp4', 'Explosion014_x264.mp4', 'Explosion015_x264.mp4', 'Explosion016_x264.mp4', 'Explosion017_x264.mp4', 'Explosion018_x264.mp4', 'Explosion019_x264.mp4', 'Explosion020_x264.mp4', 'Explosion021_x264.mp4', 'Explosion022_x264.mp4', 'Explosion023_x264.mp4', 'Explosion024_x264.mp4', 'Explosion025_x264.mp4', 'Explosion026_x264.mp4', 'Explosion027_x264.mp4', 'Explosion028_x264.mp4', 'Explosion029_x264.mp4', 'Explosion030_x264.mp4', 'Explosion032_x264.mp4', 'Explosion033_x264.mp4', 'Explosion034_x264.mp4', 'Explosion035_x264.mp4', 'Explosion036_x264.mp4', 'Explosion037_x264.mp4', 'Explosion038_x264.mp4', 'Explosion039_x264.mp4', 'Explosion040_x264.mp4', 'Explosion041_x264.mp4', 'Explosion042_x264.mp4', 'Explosion043_x264.mp4', 'Explosion044_x264.mp4', 'Explosion045_x264.mp4', 'Explosion046_x264.mp4', 'Explosion047_x264.mp4', 'Explosion048_x264.mp4', 'Explosion050_x264.mp4', 'Explosion051_x264.mp4', 'Explosion052_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Fighting002_x264.mp4', 'Fighting003_x264.mp4', 'Fighting004_x264.mp4', 'Fighting005_x264.mp4', 'Fighting006_x264.mp4', 'Fighting007_x264.mp4', 'Fighting008_x264.mp4', 'Fighting009_x264.mp4', 'Fighting010_x264.mp4', 'Fighting011_x264.mp4', 'Fighting012_x264.mp4', 'Fighting013_x264.mp4', 'Fighting014_x264.mp4', 'Fighting015_x264.mp4', 'Fighting016_x264.mp4', 'Fighting017_x264.mp4', 'Fighting018_x264.mp4', 'Fighting019_x264.mp4', 'Fighting020_x264.mp4', 'Fighting021_x264.mp4', 'Fighting022_x264.mp4', 'Fighting023_x264.mp4', 'Fighting024_x264.mp4', 'Fighting025_x264.mp4', 'Fighting026_x264.mp4', 'Fighting027_x264.mp4', 'Fighting028_x264.mp4', 'Fighting029_x264.mp4', 'Fighting030_x264.mp4', 'Fighting031_x264.mp4', 'Fighting032_x264.mp4', 'Fighting033_x264.mp4', 'Fighting034_x264.mp4', 'Fighting035_x264.mp4', 'Fighting036_x264.mp4', 'Fighting037_x264.mp4', 'Fighting038_x264.mp4', 'Fighting039_x264.mp4', 'Fighting040_x264.mp4', 'Fighting041_x264.mp4', 'Fighting042_x264.mp4', 'Fighting043_x264.mp4', 'Fighting044_x264.mp4', 'Fighting045_x264.mp4', 'Fighting046_x264.mp4', 'Fighting047_x264.mp4', 'Fighting048_x264.mp4', 'Fighting049_x264.mp4', 'Fighting050_x264.mp4', 'Fighting051_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['RoadAccidents001_x264.mp4', 'RoadAccidents002_x264.mp4', 'RoadAccidents003_x264.mp4', 'RoadAccidents004_x264.mp4', 'RoadAccidents005_x264.mp4', 'RoadAccidents006_x264.mp4', 'RoadAccidents007_x264.mp4', 'RoadAccidents008_x264.mp4', 'RoadAccidents009_x264.mp4', 'RoadAccidents010_x264.mp4', 'RoadAccidents011_x264.mp4', 'RoadAccidents012_x264.mp4', 'RoadAccidents013_x264.mp4', 'RoadAccidents014_x264.mp4', 'RoadAccidents015_x264.mp4', 'RoadAccidents016_x264.mp4', 'RoadAccidents017_x264.mp4', 'RoadAccidents018_x264.mp4', 'RoadAccidents019_x264.mp4', 'RoadAccidents020_x264.mp4', 'RoadAccidents021_x264.mp4', 'RoadAccidents022_x264.mp4', 'RoadAccidents023_x264.mp4', 'RoadAccidents024_x264.mp4', 'RoadAccidents025_x264.mp4', 'RoadAccidents026_x264.mp4', 'RoadAccidents027_x264.mp4', 'RoadAccidents028_x264.mp4', 'RoadAccidents029_x264.mp4', 'RoadAccidents030_x264.mp4', 'RoadAccidents031_x264.mp4', 'RoadAccidents032_x264.mp4', 'RoadAccidents033_x264.mp4', 'RoadAccidents034_x264.mp4', 'RoadAccidents035_x264.mp4', 'RoadAccidents036_x264.mp4', 'RoadAccidents037_x264.mp4', 'RoadAccidents038_x264.mp4', 'RoadAccidents039_x264.mp4', 'RoadAccidents040_x264.mp4', 'RoadAccidents041_x264.mp4', 'RoadAccidents042_x264.mp4', 'RoadAccidents043_x264.mp4', 'RoadAccidents044_x264.mp4', 'RoadAccidents046_x264.mp4', 'RoadAccidents047_x264.mp4', 'RoadAccidents048_x264.mp4', 'RoadAccidents049_x264.mp4', 'RoadAccidents050_x264.mp4', 'RoadAccidents051_x264.mp4', 'RoadAccidents052_x264.mp4', 'RoadAccidents053_x264.mp4', 'RoadAccidents054_x264.mp4', 'RoadAccidents055_x264.mp4', 'RoadAccidents056_x264.mp4', 'RoadAccidents057_x264.mp4', 'RoadAccidents058_x264.mp4', 'RoadAccidents059_x264.mp4', 'RoadAccidents060_x264.mp4', 'RoadAccidents061_x264.mp4', 'RoadAccidents062_x264.mp4', 'RoadAccidents063_x264.mp4', 'RoadAccidents064_x264.mp4', 'RoadAccidents065_x264.mp4', 'RoadAccidents066_x264.mp4', 'RoadAccidents067_x264.mp4', 'RoadAccidents068_x264.mp4', 'RoadAccidents069_x264.mp4', 'RoadAccidents070_x264.mp4', 'RoadAccidents071_x264.mp4', 'RoadAccidents072_x264.mp4', 'RoadAccidents073_x264.mp4', 'RoadAccidents074_x264.mp4', 'RoadAccidents075_x264.mp4', 'RoadAccidents076_x264.mp4', 'RoadAccidents077_x264.mp4', 'RoadAccidents078_x264.mp4', 'RoadAccidents079_x264.mp4', 'RoadAccidents080_x264.mp4', 'RoadAccidents081_x264.mp4', 'RoadAccidents082_x264.mp4', 'RoadAccidents083_x264.mp4', 'RoadAccidents084_x264.mp4', 'RoadAccidents085_x264.mp4', 'RoadAccidents086_x264.mp4', 'RoadAccidents087_x264.mp4', 'RoadAccidents088_x264.mp4', 'RoadAccidents089_x264.mp4', 'RoadAccidents090_x264.mp4', 'RoadAccidents091_x264.mp4', 'RoadAccidents092_x264.mp4', 'RoadAccidents093_x264.mp4', 'RoadAccidents094_x264.mp4', 'RoadAccidents095_x264.mp4', 'RoadAccidents096_x264.mp4', 'RoadAccidents097_x264.mp4', 'RoadAccidents098_x264.mp4', 'RoadAccidents099_x264.mp4', 'RoadAccidents100_x264.mp4', 'RoadAccidents101_x264.mp4', 'RoadAccidents102_x264.mp4', 'RoadAccidents103_x264.mp4', 'RoadAccidents104_x264.mp4', 'RoadAccidents105_x264.mp4', 'RoadAccidents106_x264.mp4', 'RoadAccidents107_x264.mp4', 'RoadAccidents108_x264.mp4', 'RoadAccidents109_x264.mp4', 'RoadAccidents110_x264.mp4', 'RoadAccidents111_x264.mp4', 'RoadAccidents112_x264.mp4', 'RoadAccidents113_x264.mp4', 'RoadAccidents114_x264.mp4', 'RoadAccidents115_x264.mp4', 'RoadAccidents116_x264.mp4', 'RoadAccidents117_x264.mp4', 'RoadAccidents118_x264.mp4', 'RoadAccidents119_x264.mp4', 'RoadAccidents120_x264.mp4', 'RoadAccidents121_x264.mp4', 'RoadAccidents122_x264.mp4', 'RoadAccidents123_x264.mp4', 'RoadAccidents124_x264.mp4', 'RoadAccidents125_x264.mp4', 'RoadAccidents126_x264.mp4', 'RoadAccidents127_x264.mp4', 'RoadAccidents128_x264.mp4', 'RoadAccidents129_x264.mp4', 'RoadAccidents130_x264.mp4', 'RoadAccidents131_x264.mp4', 'RoadAccidents132_x264.mp4', 'RoadAccidents133_x264.mp4', 'RoadAccidents134_x264.mp4', 'RoadAccidents135_x264.mp4', 'RoadAccidents136_x264.mp4', 'RoadAccidents137_x264.mp4', 'RoadAccidents138_x264.mp4', 'RoadAccidents139_x264.mp4', 'RoadAccidents140_x264.mp4', 'RoadAccidents141_x264.mp4', 'RoadAccidents142_x264.mp4', 'RoadAccidents143_x264.mp4', 'RoadAccidents144_x264.mp4', 'RoadAccidents145_x264.mp4', 'RoadAccidents146_x264.mp4', 'RoadAccidents147_x264.mp4', 'RoadAccidents148_x264.mp4', 'RoadAccidents149_x264.mp4', 'RoadAccidents150_x264.mp4', 'RoadAccidents151_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Robbery001_x264.mp4', 'Robbery002_x264.mp4', 'Robbery003_x264.mp4', 'Robbery004_x264.mp4', 'Robbery005_x264.mp4', 'Robbery006_x264.mp4', 'Robbery007_x264.mp4', 'Robbery008_x264.mp4', 'Robbery009_x264.mp4', 'Robbery010_x264.mp4', 'Robbery011_x264.mp4', 'Robbery012_x264.mp4', 'Robbery013_x264.mp4', 'Robbery014_x264.mp4', 'Robbery015_x264.mp4', 'Robbery016_x264.mp4', 'Robbery017_x264.mp4', 'Robbery018_x264.mp4', 'Robbery019_x264.mp4', 'Robbery020_x264.mp4', 'Robbery021_x264.mp4', 'Robbery022_x264.mp4', 'Robbery023_x264.mp4', 'Robbery024_x264.mp4', 'Robbery025_x264.mp4', 'Robbery026_x264.mp4', 'Robbery027_x264.mp4', 'Robbery028_x264.mp4', 'Robbery029_x264.mp4', 'Robbery030_x264.mp4', 'Robbery031_x264.mp4', 'Robbery032_x264.mp4', 'Robbery033_x264.mp4', 'Robbery034_x264.mp4', 'Robbery035_x264.mp4', 'Robbery036_x264.mp4', 'Robbery037_x264.mp4', 'Robbery038_x264.mp4', 'Robbery039_x264.mp4', 'Robbery040_x264.mp4', 'Robbery041_x264.mp4', 'Robbery042_x264.mp4', 'Robbery043_x264.mp4', 'Robbery044_x264.mp4', 'Robbery045_x264.mp4', 'Robbery046_x264.mp4', 'Robbery047_x264.mp4', 'Robbery048_x264.mp4', 'Robbery049_x264.mp4', 'Robbery050_x264.mp4', 'Robbery051_x264.mp4', 'Robbery052_x264.mp4', 'Robbery053_x264.mp4', 'Robbery054_x264.mp4', 'Robbery055_x264.mp4', 'Robbery056_x264.mp4', 'Robbery057_x264.mp4', 'Robbery058_x264.mp4', 'Robbery059_x264.mp4', 'Robbery060_x264.mp4', 'Robbery061_x264.mp4', 'Robbery062_x264.mp4', 'Robbery063_x264.mp4', 'Robbery064_x264.mp4', 'Robbery065_x264.mp4', 'Robbery066_x264.mp4', 'Robbery067_x264.mp4', 'Robbery068_x264.mp4', 'Robbery069_x264.mp4', 'Robbery070_x264.mp4', 'Robbery071_x264.mp4', 'Robbery072_x264.mp4', 'Robbery073_x264.mp4', 'Robbery074_x264.mp4', 'Robbery075_x264.mp4', 'Robbery076_x264.mp4', 'Robbery077_x264.mp4', 'Robbery078_x264.mp4', 'Robbery079_x264.mp4', 'Robbery080_x264.mp4', 'Robbery081_x264.mp4', 'Robbery082_x264.mp4', 'Robbery083_x264.mp4', 'Robbery084_x264.mp4', 'Robbery085_x264.mp4', 'Robbery086_x264.mp4', 'Robbery087_x264.mp4', 'Robbery088_x264.mp4', 'Robbery089_x264.mp4', 'Robbery090_x264.mp4', 'Robbery091_x264.mp4', 'Robbery092_x264.mp4', 'Robbery093_x264.mp4', 'Robbery094_x264.mp4', 'Robbery095_x264.mp4', 'Robbery096_x264.mp4', 'Robbery097_x264.mp4', 'Robbery098_x264.mp4', 'Robbery099_x264.mp4', 'Robbery100_x264.mp4', 'Robbery101_x264.mp4', 'Robbery102_x264.mp4', 'Robbery103_x264.mp4', 'Robbery104_x264.mp4', 'Robbery105_x264.mp4', 'Robbery106_x264.mp4', 'Robbery107_x264.mp4', 'Robbery108_x264.mp4', 'Robbery109_x264.mp4', 'Robbery110_x264.mp4', 'Robbery111_x264.mp4', 'Robbery112_x264.mp4', 'Robbery113_x264.mp4', 'Robbery114_x264.mp4', 'Robbery115_x264.mp4', 'Robbery116_x264.mp4', 'Robbery117_x264.mp4', 'Robbery118_x264.mp4', 'Robbery119_x264.mp4', 'Robbery120_x264.mp4', 'Robbery121_x264.mp4', 'Robbery122_x264.mp4', 'Robbery123_x264.mp4', 'Robbery124_x264.mp4', 'Robbery125_x264.mp4', 'Robbery126_x264.mp4', 'Robbery127_x264.mp4', 'Robbery128_x264.mp4', 'Robbery129_x264.mp4', 'Robbery130_x264.mp4', 'Robbery131_x264.mp4', 'Robbery132_x264.mp4', 'Robbery133_x264.mp4', 'Robbery134_x264.mp4', 'Robbery135_x264.mp4', 'Robbery136_x264.mp4', 'Robbery137_x264.mp4', 'Robbery138_x264.mp4', 'Robbery139_x264.mp4', 'Robbery140_x264.mp4', 'Robbery141_x264.mp4', 'Robbery142_x264.mp4', 'Robbery143_x264.mp4', 'Robbery144_x264.mp4', 'Robbery145_x264.mp4', 'Robbery146_x264.mp4', 'Robbery147_x264.mp4', 'Robbery148_x264.mp4', 'Robbery149_x264.mp4', 'Robbery150_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Shooting001_x264.mp4', 'Shooting002_x264.mp4', 'Shooting003_x264.mp4', 'Shooting004_x264.mp4', 'Shooting005_x264.mp4', 'Shooting006_x264.mp4', 'Shooting007_x264.mp4', 'Shooting008_x264.mp4', 'Shooting009_x264.mp4', 'Shooting010_x264.mp4', 'Shooting011_x264.mp4', 'Shooting012_x264.mp4', 'Shooting013_x264.mp4', 'Shooting014_x264.mp4', 'Shooting015_x264.mp4', 'Shooting017_x264.mp4', 'Shooting018_x264.mp4', 'Shooting019_x264.mp4', 'Shooting020_x264.mp4', 'Shooting021_x264.mp4', 'Shooting022_x264.mp4', 'Shooting023_x264.mp4', 'Shooting024_x264.mp4', 'Shooting025_x264.mp4', 'Shooting026_x264.mp4', 'Shooting027_x264.mp4', 'Shooting028_x264.mp4', 'Shooting029_x264.mp4', 'Shooting030_x264.mp4', 'Shooting031_x264.mp4', 'Shooting032_x264.mp4', 'Shooting033_x264.mp4', 'Shooting034_x264.mp4', 'Shooting036_x264.mp4', 'Shooting037_x264.mp4', 'Shooting038_x264.mp4', 'Shooting039_x264.mp4', 'Shooting040_x264.mp4', 'Shooting041_x264.mp4', 'Shooting042_x264.mp4', 'Shooting043_x264.mp4', 'Shooting044_x264.mp4', 'Shooting046_x264.mp4', 'Shooting047_x264.mp4', 'Shooting048_x264.mp4', 'Shooting050_x264.mp4', 'Shooting051_x264.mp4', 'Shooting052_x264.mp4', 'Shooting053_x264.mp4', 'Shooting054_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Shoplifting001_x264.mp4', 'Shoplifting003_x264.mp4', 'Shoplifting004_x264.mp4', 'Shoplifting005_x264.mp4', 'Shoplifting006_x264.mp4', 'Shoplifting007_x264.mp4', 'Shoplifting008_x264.mp4', 'Shoplifting009_x264.mp4', 'Shoplifting010_x264.mp4', 'Shoplifting012_x264.mp4', 'Shoplifting013_x264.mp4', 'Shoplifting014_x264.mp4', 'Shoplifting015_x264.mp4', 'Shoplifting016_x264.mp4', 'Shoplifting017_x264.mp4', 'Shoplifting018_x264.mp4', 'Shoplifting019_x264.mp4', 'Shoplifting020_x264.mp4', 'Shoplifting021_x264.mp4', 'Shoplifting022_x264.mp4', 'Shoplifting024_x264.mp4', 'Shoplifting025_x264.mp4', 'Shoplifting026_x264.mp4', 'Shoplifting027_x264.mp4', 'Shoplifting028_x264.mp4', 'Shoplifting029_x264.mp4', 'Shoplifting030_x264.mp4', 'Shoplifting031_x264.mp4', 'Shoplifting032_x264.mp4', 'Shoplifting033_x264.mp4', 'Shoplifting034_x264.mp4', 'Shoplifting036_x264.mp4', 'Shoplifting037_x264.mp4', 'Shoplifting038_x264.mp4', 'Shoplifting039_x264.mp4', 'Shoplifting040_x264.mp4', 'Shoplifting041_x264.mp4', 'Shoplifting042_x264.mp4', 'Shoplifting043_x264.mp4', 'Shoplifting044_x264.mp4', 'Shoplifting045_x264.mp4', 'Shoplifting047_x264.mp4', 'Shoplifting048_x264.mp4', 'Shoplifting049_x264.mp4', 'Shoplifting050_x264.mp4', 'Shoplifting051_x264.mp4', 'Shoplifting052_x264.mp4', 'Shoplifting053_x264.mp4', 'Shoplifting054_x264.mp4', 'Shoplifting055_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Stealing002_x264.mp4', 'Stealing003_x264.mp4', 'Stealing004_x264.mp4', 'Stealing006_x264.mp4', 'Stealing007_x264.mp4', 'Stealing008_x264.mp4', 'Stealing009_x264.mp4', 'Stealing010_x264.mp4', 'Stealing011_x264.mp4', 'Stealing012_x264.mp4', 'Stealing013_x264.mp4', 'Stealing014_x264.mp4', 'Stealing015_x264.mp4', 'Stealing016_x264.mp4', 'Stealing017_x264.mp4', 'Stealing018_x264.mp4', 'Stealing019_x264.mp4', 'Stealing020_x264.mp4', 'Stealing021_x264.mp4', 'Stealing022_x264.mp4', 'Stealing023_x264.mp4', 'Stealing024_x264.mp4', 'Stealing025_x264.mp4', 'Stealing026_x264.mp4', 'Stealing027_x264.mp4', 'Stealing028_x264.mp4', 'Stealing029_x264.mp4', 'Stealing030_x264.mp4', 'Stealing031_x264.mp4', 'Stealing032_x264.mp4', 'Stealing035_x264.mp4', 'Stealing036_x264.mp4', 'Stealing037_x264.mp4', 'Stealing042_x264.mp4', 'Stealing043_x264.mp4', 'Stealing044_x264.mp4', 'Stealing045_x264.mp4', 'Stealing046_x264.mp4', 'Stealing047_x264.mp4', 'Stealing048_x264.mp4', 'Stealing049_x264.mp4', 'Stealing050_x264.mp4', 'Stealing051_x264.mp4', 'Stealing052_x264.mp4', 'Stealing053_x264.mp4', 'Stealing054_x264.mp4', 'Stealing055_x264.mp4', 'Stealing057_x264.mp4', 'Stealing058_x264.mp4', 'Stealing059_x264.mp4', 'Stealing060_x264.mp4', 'Stealing061_x264.mp4', 'Stealing062_x264.mp4', 'Stealing063_x264.mp4', 'Stealing065_x264.mp4', 'Stealing066_x264.mp4', 'Stealing067_x264.mp4', 'Stealing068_x264.mp4', 'Stealing069_x264.mp4', 'Stealing070_x264.mp4', 'Stealing071_x264.mp4', 'Stealing072_x264.mp4', 'Stealing073_x264.mp4', 'Stealing074_x264.mp4', 'Stealing075_x264.mp4', 'Stealing077_x264.mp4', 'Stealing078_x264.mp4', 'Stealing079_x264.mp4', 'Stealing080_x264.mp4', 'Stealing081_x264.mp4', 'Stealing082_x264.mp4', 'Stealing083_x264.mp4', 'Stealing084_x264.mp4', 'Stealing086_x264.mp4', 'Stealing087_x264.mp4', 'Stealing088_x264.mp4', 'Stealing089_x264.mp4', 'Stealing091_x264.mp4', 'Stealing092_x264.mp4', 'Stealing093_x264.mp4', 'Stealing094_x264.mp4', 'Stealing095_x264.mp4', 'Stealing096_x264.mp4', 'Stealing097_x264.mp4', 'Stealing098_x264.mp4', 'Stealing100_x264.mp4', 'Stealing101_x264.mp4', 'Stealing102_x264.mp4', 'Stealing103_x264.mp4', 'Stealing104_x264.mp4', 'Stealing105_x264.mp4', 'Stealing106_x264.mp4', 'Stealing107_x264.mp4', 'Stealing108_x264.mp4', 'Stealing109_x264.mp4', 'Stealing110_x264.mp4', 'Stealing111_x264.mp4', 'Stealing112_x264.mp4', 'Stealing113_x264.mp4', 'Stealing114_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Vandalism001_x264.mp4', 'Vandalism002_x264.mp4', 'Vandalism003_x264.mp4', 'Vandalism004_x264.mp4', 'Vandalism005_x264.mp4', 'Vandalism006_x264.mp4', 'Vandalism007_x264.mp4', 'Vandalism008_x264.mp4', 'Vandalism009_x264.mp4', 'Vandalism010_x264.mp4', 'Vandalism011_x264.mp4', 'Vandalism012_x264.mp4', 'Vandalism013_x264.mp4', 'Vandalism014_x264.mp4', 'Vandalism015_x264.mp4', 'Vandalism016_x264.mp4', 'Vandalism017_x264.mp4', 'Vandalism018_x264.mp4', 'Vandalism019_x264.mp4', 'Vandalism020_x264.mp4', 'Vandalism021_x264.mp4', 'Vandalism022_x264.mp4', 'Vandalism023_x264.mp4', 'Vandalism024_x264.mp4', 'Vandalism025_x264.mp4', 'Vandalism026_x264.mp4', 'Vandalism027_x264.mp4', 'Vandalism028_x264.mp4', 'Vandalism029_x264.mp4', 'Vandalism030_x264.mp4', 'Vandalism031_x264.mp4', 'Vandalism032_x264.mp4', 'Vandalism033_x264.mp4', 'Vandalism034_x264.mp4', 'Vandalism035_x264.mp4', 'Vandalism036_x264.mp4', 'Vandalism037_x264.mp4', 'Vandalism038_x264.mp4', 'Vandalism039_x264.mp4', 'Vandalism040_x264.mp4', 'Vandalism041_x264.mp4', 'Vandalism042_x264.mp4', 'Vandalism043_x264.mp4', 'Vandalism044_x264.mp4', 'Vandalism045_x264.mp4', 'Vandalism046_x264.mp4', 'Vandalism047_x264.mp4', 'Vandalism048_x264.mp4', 'Vandalism049_x264.mp4', 'Vandalism050_x264.mp4']\n",
      "경로가 존재합니다.\n",
      "디렉터리 내용: ['Normal_Videos001_x264.mp4', 'Normal_Videos002_x264.mp4', 'Normal_Videos004_x264.mp4', 'Normal_Videos005_x264.mp4', 'Normal_Videos007_x264.mp4', 'Normal_Videos008_x264.mp4', 'Normal_Videos009_x264.mp4', 'Normal_Videos011_x264.mp4', 'Normal_Videos012_x264.mp4', 'Normal_Videos013_x264.mp4', 'Normal_Videos016_x264.mp4', 'Normal_Videos017_x264.mp4', 'Normal_Videos020_x264.mp4', 'Normal_Videos021_x264.mp4', 'Normal_Videos022_x264.mp4', 'Normal_Videos023_x264.mp4', 'Normal_Videos026_x264.mp4', 'Normal_Videos028_x264.mp4', 'Normal_Videos029_x264.mp4', 'Normal_Videos030_x264.mp4', 'Normal_Videos031_x264.mp4', 'Normal_Videos032_x264.mp4', 'Normal_Videos035_x264.mp4', 'Normal_Videos036_x264.mp4', 'Normal_Videos037_x264.mp4', 'Normal_Videos038_x264.mp4', 'Normal_Videos039_x264.mp4', 'Normal_Videos040_x264.mp4', 'Normal_Videos043_x264.mp4', 'Normal_Videos044_x264.mp4', 'Normal_Videos045_x264.mp4', 'Normal_Videos046_x264.mp4', 'Normal_Videos047_x264.mp4', 'Normal_Videos049_x264.mp4', 'Normal_Videos052_x264.mp4', 'Normal_Videos053_x264.mp4', 'Normal_Videos054_x264.mp4', 'Normal_Videos055_x264.mp4', 'Normal_Videos057_x264.mp4', 'Normal_Videos058_x264.mp4', 'Normal_Videos060_x264.mp4', 'Normal_Videos061_x264.mp4', 'Normal_Videos062_x264.mp4', 'Normal_Videos064_x264.mp4', 'Normal_Videos065_x264.mp4', 'Normal_Videos066_x264.mp4', 'Normal_Videos068_x264.mp4', 'Normal_Videos069_x264.mp4', 'Normal_Videos071_x264.mp4', 'Normal_Videos072_x264.mp4', 'Normal_Videos073_x264.mp4', 'Normal_Videos074_x264.mp4', 'Normal_Videos075_x264.mp4', 'Normal_Videos076_x264.mp4', 'Normal_Videos077_x264.mp4', 'Normal_Videos078_x264.mp4', 'Normal_Videos079_x264.mp4', 'Normal_Videos080_x264.mp4', 'Normal_Videos081_x264.mp4', 'Normal_Videos082_x264.mp4', 'Normal_Videos083_x264.mp4', 'Normal_Videos084_x264.mp4', 'Normal_Videos085_x264.mp4', 'Normal_Videos086_x264.mp4', 'Normal_Videos087_x264.mp4', 'Normal_Videos088_x264.mp4', 'Normal_Videos089_x264.mp4', 'Normal_Videos090_x264.mp4', 'Normal_Videos091_x264.mp4', 'Normal_Videos092_x264.mp4', 'Normal_Videos093_x264.mp4', 'Normal_Videos094_x264.mp4', 'Normal_Videos095_x264.mp4', 'Normal_Videos096_x264.mp4', 'Normal_Videos097_x264.mp4', 'Normal_Videos098_x264.mp4', 'Normal_Videos099_x264.mp4', 'Normal_Videos101_x264.mp4', 'Normal_Videos102_x264.mp4', 'Normal_Videos103_x264.mp4', 'Normal_Videos104_x264.mp4', 'Normal_Videos105_x264.mp4', 'Normal_Videos106_x264.mp4', 'Normal_Videos107_x264.mp4', 'Normal_Videos108_x264.mp4', 'Normal_Videos109_x264.mp4', 'Normal_Videos110_x264.mp4', 'Normal_Videos111_x264.mp4', 'Normal_Videos112_x264.mp4', 'Normal_Videos113_x264.mp4', 'Normal_Videos114_x264.mp4', 'Normal_Videos115_x264.mp4', 'Normal_Videos116_x264.mp4', 'Normal_Videos117_x264.mp4', 'Normal_Videos118_x264.mp4', 'Normal_Videos119_x264.mp4', 'Normal_Videos120_x264.mp4', 'Normal_Videos121_x264.mp4', 'Normal_Videos122_x264.mp4', 'Normal_Videos123_x264.mp4', 'Normal_Videos124_x264.mp4', 'Normal_Videos125_x264.mp4', 'Normal_Videos126_x264.mp4', 'Normal_Videos127_x264.mp4', 'Normal_Videos128_x264.mp4', 'Normal_Videos130_x264.mp4', 'Normal_Videos131_x264.mp4', 'Normal_Videos132_x264.mp4', 'Normal_Videos133_x264.mp4', 'Normal_Videos134_x264.mp4', 'Normal_Videos135_x264.mp4', 'Normal_Videos136_x264.mp4', 'Normal_Videos137_x264.mp4', 'Normal_Videos138_x264.mp4', 'Normal_Videos139_x264.mp4', 'Normal_Videos140_x264.mp4', 'Normal_Videos141_x264.mp4', 'Normal_Videos142_x264.mp4', 'Normal_Videos143_x264.mp4', 'Normal_Videos144_x264.mp4', 'Normal_Videos145_x264.mp4', 'Normal_Videos146_x264.mp4', 'Normal_Videos147_x264.mp4', 'Normal_Videos148_x264.mp4', 'Normal_Videos149_x264.mp4', 'Normal_Videos151_x264.mp4', 'Normal_Videos152_x264.mp4', 'Normal_Videos153_x264.mp4', 'Normal_Videos154_x264.mp4', 'Normal_Videos155_x264.mp4', 'Normal_Videos156_x264.mp4', 'Normal_Videos157_x264.mp4', 'Normal_Videos158_x264.mp4', 'Normal_Videos159_x264.mp4', 'Normal_Videos160_x264.mp4', 'Normal_Videos161_x264.mp4', 'Normal_Videos162_x264.mp4', 'Normal_Videos163_x264.mp4', 'Normal_Videos164_x264.mp4', 'Normal_Videos165_x264.mp4', 'Normal_Videos166_x264.mp4', 'Normal_Videos167_x264.mp4', 'Normal_Videos169_x264.mp4', 'Normal_Videos170_x264.mp4', 'Normal_Videos171_x264.mp4', 'Normal_Videos172_x264.mp4', 'Normal_Videos173_x264.mp4', 'Normal_Videos174_x264.mp4', 'Normal_Videos176_x264.mp4', 'Normal_Videos177_x264.mp4', 'Normal_Videos178_x264.mp4', 'Normal_Videos179_x264.mp4', 'Normal_Videos180_x264.mp4', 'Normal_Videos181_x264.mp4', 'Normal_Videos183_x264.mp4', 'Normal_Videos184_x264.mp4', 'Normal_Videos185_x264.mp4', 'Normal_Videos186_x264.mp4', 'Normal_Videos187_x264.mp4', 'Normal_Videos188_x264.mp4', 'Normal_Videos190_x264.mp4', 'Normal_Videos191_x264.mp4', 'Normal_Videos192_x264.mp4', 'Normal_Videos193_x264.mp4', 'Normal_Videos194_x264.mp4', 'Normal_Videos195_x264.mp4', 'Normal_Videos197_x264.mp4', 'Normal_Videos198_x264.mp4', 'Normal_Videos199_x264.mp4', 'Normal_Videos200_x264.mp4', 'Normal_Videos201_x264.mp4', 'Normal_Videos202_x264.mp4', 'Normal_Videos204_x264.mp4', 'Normal_Videos205_x264.mp4', 'Normal_Videos206_x264.mp4', 'Normal_Videos207_x264.mp4', 'Normal_Videos208_x264.mp4', 'Normal_Videos209_x264.mp4', 'Normal_Videos211_x264.mp4', 'Normal_Videos212_x264.mp4', 'Normal_Videos213_x264.mp4', 'Normal_Videos214_x264.mp4', 'Normal_Videos215_x264.mp4', 'Normal_Videos216_x264.mp4', 'Normal_Videos218_x264.mp4', 'Normal_Videos219_x264.mp4', 'Normal_Videos220_x264.mp4', 'Normal_Videos221_x264.mp4', 'Normal_Videos222_x264.mp4', 'Normal_Videos223_x264.mp4', 'Normal_Videos225_x264.mp4', 'Normal_Videos226_x264.mp4', 'Normal_Videos227_x264.mp4', 'Normal_Videos228_x264.mp4', 'Normal_Videos229_x264.mp4', 'Normal_Videos230_x264.mp4', 'Normal_Videos231_x264.mp4', 'Normal_Videos232_x264.mp4', 'Normal_Videos233_x264.mp4', 'Normal_Videos234_x264.mp4', 'Normal_Videos235_x264.mp4', 'Normal_Videos236_x264.mp4', 'Normal_Videos237_x264.mp4', 'Normal_Videos238_x264.mp4', 'Normal_Videos239_x264.mp4', 'Normal_Videos240_x264.mp4', 'Normal_Videos241_x264.mp4', 'Normal_Videos242_x264.mp4', 'Normal_Videos243_x264.mp4', 'Normal_Videos244_x264.mp4', 'Normal_Videos245_x264.mp4', 'Normal_Videos249_x264.mp4', 'Normal_Videos250_x264.mp4', 'Normal_Videos252_x264.mp4', 'Normal_Videos253_x264.mp4', 'Normal_Videos254_x264.mp4', 'Normal_Videos255_x264.mp4', 'Normal_Videos256_x264.mp4', 'Normal_Videos257_x264.mp4', 'Normal_Videos258_x264.mp4', 'Normal_Videos259_x264.mp4', 'Normal_Videos260_x264.mp4', 'Normal_Videos261_x264.mp4', 'Normal_Videos262_x264.mp4', 'Normal_Videos263_x264.mp4', 'Normal_Videos264_x264.mp4', 'Normal_Videos265_x264.mp4', 'Normal_Videos266_x264.mp4', 'Normal_Videos267_x264.mp4', 'Normal_Videos268_x264.mp4', 'Normal_Videos269_x264.mp4', 'Normal_Videos270_x264.mp4', 'Normal_Videos271_x264.mp4', 'Normal_Videos272_x264.mp4', 'Normal_Videos273_x264.mp4', 'Normal_Videos274_x264.mp4', 'Normal_Videos275_x264.mp4', 'Normal_Videos276_x264.mp4', 'Normal_Videos277_x264.mp4', 'Normal_Videos278_x264.mp4', 'Normal_Videos279_x264.mp4', 'Normal_Videos280_x264.mp4', 'Normal_Videos281_x264.mp4', 'Normal_Videos282_x264.mp4', 'Normal_Videos283_x264.mp4', 'Normal_Videos284_x264.mp4', 'Normal_Videos285_x264.mp4', 'Normal_Videos286_x264.mp4', 'Normal_Videos287_x264.mp4', 'Normal_Videos288_x264.mp4', 'Normal_Videos290_x264.mp4', 'Normal_Videos291_x264.mp4', 'Normal_Videos292_x264.mp4', 'Normal_Videos293_x264.mp4', 'Normal_Videos294_x264.mp4', 'Normal_Videos295_x264.mp4', 'Normal_Videos296_x264.mp4', 'Normal_Videos297_x264.mp4', 'Normal_Videos298_x264.mp4', 'Normal_Videos299_x264.mp4', 'Normal_Videos300_x264.mp4', 'Normal_Videos301_x264.mp4', 'Normal_Videos302_x264.mp4', 'Normal_Videos303_x264.mp4', 'Normal_Videos304_x264.mp4', 'Normal_Videos305_x264.mp4', 'Normal_Videos306_x264.mp4', 'Normal_Videos307_x264.mp4', 'Normal_Videos308_x264.mp4', 'Normal_Videos309_x264.mp4', 'Normal_Videos311_x264.mp4', 'Normal_Videos313_x264.mp4', 'Normal_Videos314_x264.mp4', 'Normal_Videos315_x264.mp4', 'Normal_Videos316_x264.mp4', 'Normal_Videos318_x264.mp4', 'Normal_Videos319_x264.mp4', 'Normal_Videos320_x264.mp4', 'Normal_Videos321_x264.mp4', 'Normal_Videos322_x264.mp4', 'Normal_Videos323_x264.mp4', 'Normal_Videos324_x264.mp4', 'Normal_Videos325_x264.mp4', 'Normal_Videos326_x264.mp4', 'Normal_Videos327_x264.mp4', 'Normal_Videos328_x264.mp4', 'Normal_Videos329_x264.mp4', 'Normal_Videos330_x264.mp4', 'Normal_Videos331_x264.mp4', 'Normal_Videos332_x264.mp4', 'Normal_Videos333_x264.mp4', 'Normal_Videos334_x264.mp4', 'Normal_Videos335_x264.mp4', 'Normal_Videos336_x264.mp4', 'Normal_Videos337_x264.mp4', 'Normal_Videos338_x264.mp4', 'Normal_Videos339_x264.mp4', 'Normal_Videos340_x264.mp4', 'Normal_Videos341_x264.mp4', 'Normal_Videos342_x264.mp4', 'Normal_Videos343_x264.mp4', 'Normal_Videos344_x264.mp4', 'Normal_Videos346_x264.mp4', 'Normal_Videos347_x264.mp4', 'Normal_Videos348_x264.mp4', 'Normal_Videos349_x264.mp4', 'Normal_Videos350_x264.mp4', 'Normal_Videos351_x264.mp4', 'Normal_Videos353_x264.mp4', 'Normal_Videos354_x264.mp4', 'Normal_Videos355_x264.mp4', 'Normal_Videos356_x264.mp4', 'Normal_Videos357_x264.mp4', 'Normal_Videos358_x264.mp4', 'Normal_Videos359_x264.mp4', 'Normal_Videos361_x264.mp4', 'Normal_Videos362_x264.mp4', 'Normal_Videos363_x264.mp4', 'Normal_Videos364_x264.mp4', 'Normal_Videos366_x264.mp4', 'Normal_Videos367_x264.mp4', 'Normal_Videos368_x264.mp4', 'Normal_Videos369_x264.mp4', 'Normal_Videos370_x264.mp4', 'Normal_Videos371_x264.mp4', 'Normal_Videos372_x264.mp4', 'Normal_Videos373_x264.mp4', 'Normal_Videos374_x264.mp4', 'Normal_Videos375_x264.mp4', 'Normal_Videos376_x264.mp4', 'Normal_Videos377_x264.mp4', 'Normal_Videos378_x264.mp4', 'Normal_Videos379_x264.mp4', 'Normal_Videos380_x264.mp4', 'Normal_Videos381_x264.mp4', 'Normal_Videos382_x264.mp4', 'Normal_Videos383_x264.mp4', 'Normal_Videos384_x264.mp4', 'Normal_Videos385_x264.mp4', 'Normal_Videos386_x264.mp4', 'Normal_Videos387_x264.mp4', 'Normal_Videos388_x264.mp4', 'Normal_Videos389_x264.mp4', 'Normal_Videos390_x264.mp4', 'Normal_Videos391_x264.mp4', 'Normal_Videos392_x264.mp4', 'Normal_Videos393_x264.mp4', 'Normal_Videos394_x264.mp4', 'Normal_Videos395_x264.mp4', 'Normal_Videos396_x264.mp4', 'Normal_Videos397_x264.mp4', 'Normal_Videos398_x264.mp4', 'Normal_Videos399_x264.mp4', 'Normal_Videos400_x264.mp4', 'Normal_Videos402_x264.mp4', 'Normal_Videos403_x264.mp4', 'Normal_Videos404_x264.mp4', 'Normal_Videos405_x264.mp4', 'Normal_Videos406_x264.mp4', 'Normal_Videos407_x264.mp4', 'Normal_Videos408_x264.mp4', 'Normal_Videos409_x264.mp4', 'Normal_Videos410_x264.mp4', 'Normal_Videos411_x264.mp4', 'Normal_Videos412_x264.mp4', 'Normal_Videos413_x264.mp4', 'Normal_Videos414_x264.mp4', 'Normal_Videos415_x264.mp4', 'Normal_Videos416_x264.mp4', 'Normal_Videos418_x264.mp4', 'Normal_Videos419_x264.mp4', 'Normal_Videos420_x264.mp4', 'Normal_Videos421_x264.mp4', 'Normal_Videos422_x264.mp4', 'Normal_Videos423_x264.mp4', 'Normal_Videos424_x264.mp4', 'Normal_Videos425_x264.mp4', 'Normal_Videos426_x264.mp4', 'Normal_Videos427_x264.mp4', 'Normal_Videos428_x264.mp4', 'Normal_Videos429_x264.mp4', 'Normal_Videos430_x264.mp4', 'Normal_Videos431_x264.mp4', 'Normal_Videos432_x264.mp4', 'Normal_Videos433_x264.mp4', 'Normal_Videos434_x264.mp4', 'Normal_Videos435_x264.mp4', 'Normal_Videos436_x264.mp4', 'Normal_Videos437_x264.mp4', 'Normal_Videos438_x264.mp4', 'Normal_Videos440_x264.mp4', 'Normal_Videos441_x264.mp4', 'Normal_Videos442_x264.mp4', 'Normal_Videos443_x264.mp4', 'Normal_Videos444_x264.mp4', 'Normal_Videos445_x264.mp4', 'Normal_Videos446_x264.mp4', 'Normal_Videos447_x264.mp4', 'Normal_Videos448_x264.mp4', 'Normal_Videos449_x264.mp4', 'Normal_Videos450_x264.mp4', 'Normal_Videos451_x264.mp4', 'Normal_Videos454_x264.mp4', 'Normal_Videos455_x264.mp4', 'Normal_Videos456_x264.mp4', 'Normal_Videos457_x264.mp4', 'Normal_Videos458_x264.mp4', 'Normal_Videos459_x264.mp4', 'Normal_Videos460_x264.mp4', 'Normal_Videos461_x264.mp4', 'Normal_Videos462_x264.mp4', 'Normal_Videos463_x264.mp4', 'Normal_Videos464_x264.mp4', 'Normal_Videos465_x264.mp4', 'Normal_Videos466_x264.mp4', 'Normal_Videos467_x264.mp4', 'Normal_Videos468_x264.mp4', 'Normal_Videos469_x264.mp4', 'Normal_Videos470_x264.mp4', 'Normal_Videos471_x264.mp4', 'Normal_Videos472_x264.mp4', 'Normal_Videos473_x264.mp4', 'Normal_Videos474_x264.mp4', 'Normal_Videos475_x264.mp4', 'Normal_Videos476_x264.mp4', 'Normal_Videos477_x264.mp4', 'Normal_Videos479_x264.mp4', 'Normal_Videos480_x264.mp4', 'Normal_Videos481_x264.mp4', 'Normal_Videos482_x264.mp4', 'Normal_Videos483_x264.mp4', 'Normal_Videos484_x264.mp4', 'Normal_Videos485_x264.mp4', 'Normal_Videos486_x264.mp4', 'Normal_Videos487_x264.mp4', 'Normal_Videos488_x264.mp4', 'Normal_Videos489_x264.mp4', 'Normal_Videos490_x264.mp4', 'Normal_Videos491_x264.mp4', 'Normal_Videos492_x264.mp4', 'Normal_Videos493_x264.mp4', 'Normal_Videos494_x264.mp4', 'Normal_Videos495_x264.mp4', 'Normal_Videos496_x264.mp4', 'Normal_Videos497_x264.mp4', 'Normal_Videos498_x264.mp4', 'Normal_Videos499_x264.mp4', 'Normal_Videos500_x264.mp4', 'Normal_Videos501_x264.mp4', 'Normal_Videos502_x264.mp4', 'Normal_Videos503_x264.mp4', 'Normal_Videos504_x264.mp4', 'Normal_Videos505_x264.mp4', 'Normal_Videos506_x264.mp4', 'Normal_Videos507_x264.mp4', 'Normal_Videos508_x264.mp4', 'Normal_Videos509_x264.mp4', 'Normal_Videos510_x264.mp4', 'Normal_Videos511_x264.mp4', 'Normal_Videos512_x264.mp4', 'Normal_Videos513_x264.mp4', 'Normal_Videos514_x264.mp4', 'Normal_Videos515_x264.mp4', 'Normal_Videos516_x264.mp4', 'Normal_Videos517_x264.mp4', 'Normal_Videos518_x264.mp4', 'Normal_Videos519_x264.mp4', 'Normal_Videos520_x264.mp4', 'Normal_Videos521_x264.mp4', 'Normal_Videos522_x264.mp4', 'Normal_Videos523_x264.mp4', 'Normal_Videos524_x264.mp4', 'Normal_Videos525_x264.mp4', 'Normal_Videos526_x264.mp4', 'Normal_Videos527_x264.mp4', 'Normal_Videos528_x264.mp4', 'Normal_Videos529_x264.mp4', 'Normal_Videos530_x264.mp4', 'Normal_Videos531_x264.mp4', 'Normal_Videos532_x264.mp4', 'Normal_Videos533_x264.mp4', 'Normal_Videos534_x264.mp4', 'Normal_Videos535_x264.mp4', 'Normal_Videos536_x264.mp4', 'Normal_Videos537_x264.mp4', 'Normal_Videos538_x264.mp4', 'Normal_Videos539_x264.mp4', 'Normal_Videos540_x264.mp4', 'Normal_Videos541_x264.mp4', 'Normal_Videos542_x264.mp4', 'Normal_Videos543_x264.mp4', 'Normal_Videos544_x264.mp4', 'Normal_Videos545_x264.mp4', 'Normal_Videos546_x264.mp4', 'Normal_Videos547_x264.mp4', 'Normal_Videos548_x264.mp4', 'Normal_Videos549_x264.mp4', 'Normal_Videos550_x264.mp4', 'Normal_Videos551_x264.mp4', 'Normal_Videos552_x264.mp4', 'Normal_Videos553_x264.mp4', 'Normal_Videos554_x264.mp4', 'Normal_Videos555_x264.mp4', 'Normal_Videos556_x264.mp4', 'Normal_Videos557_x264.mp4', 'Normal_Videos558_x264.mp4', 'Normal_Videos559_x264.mp4', 'Normal_Videos560_x264.mp4', 'Normal_Videos561_x264.mp4', 'Normal_Videos562_x264.mp4', 'Normal_Videos563_x264.mp4', 'Normal_Videos564_x264.mp4', 'Normal_Videos565_x264.mp4', 'Normal_Videos566_x264.mp4', 'Normal_Videos567_x264.mp4', 'Normal_Videos568_x264.mp4', 'Normal_Videos569_x264.mp4', 'Normal_Videos570_x264.mp4', 'Normal_Videos571_x264.mp4', 'Normal_Videos572_x264.mp4', 'Normal_Videos573_x264.mp4', 'Normal_Videos574_x264.mp4', 'Normal_Videos575_x264.mp4', 'Normal_Videos577_x264.mp4', 'Normal_Videos578_x264.mp4', 'Normal_Videos579_x264.mp4', 'Normal_Videos580_x264.mp4', 'Normal_Videos581_x264.mp4', 'Normal_Videos582_x264.mp4', 'Normal_Videos583_x264.mp4', 'Normal_Videos584_x264.mp4', 'Normal_Videos585_x264.mp4', 'Normal_Videos586_x264.mp4', 'Normal_Videos587_x264.mp4', 'Normal_Videos588_x264.mp4', 'Normal_Videos589_x264.mp4', 'Normal_Videos590_x264.mp4', 'Normal_Videos591_x264.mp4', 'Normal_Videos592_x264.mp4', 'Normal_Videos593_x264.mp4', 'Normal_Videos594_x264.mp4', 'Normal_Videos595_x264.mp4', 'Normal_Videos596_x264.mp4', 'Normal_Videos598_x264.mp4', 'Normal_Videos599_x264.mp4', 'Normal_Videos600_x264.mp4', 'Normal_Videos601_x264.mp4', 'Normal_Videos602_x264.mp4', 'Normal_Videos604_x264.mp4', 'Normal_Videos605_x264.mp4', 'Normal_Videos607_x264.mp4', 'Normal_Videos608_x264.mp4', 'Normal_Videos609_x264.mp4', 'Normal_Videos610_x264.mp4', 'Normal_Videos611_x264.mp4', 'Normal_Videos612_x264.mp4', 'Normal_Videos613_x264.mp4', 'Normal_Videos614_x264.mp4', 'Normal_Videos615_x264.mp4', 'Normal_Videos616_x264.mp4', 'Normal_Videos617_x264.mp4', 'Normal_Videos618_x264.mp4', 'Normal_Videos619_x264.mp4', 'Normal_Videos620_x264.mp4', 'Normal_Videos622_x264.mp4', 'Normal_Videos623_x264.mp4', 'Normal_Videos624_x264.mp4', 'Normal_Videos625_x264.mp4', 'Normal_Videos626_x264.mp4', 'Normal_Videos627_x264.mp4', 'Normal_Videos628_x264.mp4', 'Normal_Videos629_x264.mp4', 'Normal_Videos630_x264.mp4', 'Normal_Videos631_x264.mp4', 'Normal_Videos632_x264.mp4', 'Normal_Videos633_x264.mp4', 'Normal_Videos635_x264.mp4', 'Normal_Videos636_x264.mp4', 'Normal_Videos637_x264.mp4', 'Normal_Videos638_x264.mp4', 'Normal_Videos639_x264.mp4', 'Normal_Videos640_x264.mp4', 'Normal_Videos642_x264.mp4', 'Normal_Videos643_x264.mp4', 'Normal_Videos644_x264.mp4', 'Normal_Videos645_x264.mp4', 'Normal_Videos646_x264.mp4', 'Normal_Videos647_x264.mp4', 'Normal_Videos648_x264.mp4', 'Normal_Videos649_x264.mp4', 'Normal_Videos650_x264.mp4', 'Normal_Videos651_x264.mp4', 'Normal_Videos652_x264.mp4', 'Normal_Videos653_x264.mp4', 'Normal_Videos654_x264.mp4', 'Normal_Videos655_x264.mp4', 'Normal_Videos657_x264.mp4', 'Normal_Videos658_x264.mp4', 'Normal_Videos659_x264.mp4', 'Normal_Videos660_x264.mp4', 'Normal_Videos661_x264.mp4', 'Normal_Videos662_x264.mp4', 'Normal_Videos663_x264.mp4', 'Normal_Videos664_x264.mp4', 'Normal_Videos665_x264.mp4', 'Normal_Videos666_x264.mp4', 'Normal_Videos667_x264.mp4', 'Normal_Videos668_x264.mp4', 'Normal_Videos669_x264.mp4', 'Normal_Videos670_x264.mp4', 'Normal_Videos671_x264.mp4', 'Normal_Videos672_x264.mp4', 'Normal_Videos673_x264.mp4', 'Normal_Videos674_x264.mp4', 'Normal_Videos675_x264.mp4', 'Normal_Videos676_x264.mp4', 'Normal_Videos677_x264.mp4', 'Normal_Videos678_x264.mp4', 'Normal_Videos679_x264.mp4', 'Normal_Videos680_x264.mp4', 'Normal_Videos681_x264.mp4', 'Normal_Videos682_x264.mp4', 'Normal_Videos683_x264.mp4', 'Normal_Videos684_x264.mp4', 'Normal_Videos685_x264.mp4', 'Normal_Videos687_x264.mp4', 'Normal_Videos688_x264.mp4', 'Normal_Videos689_x264.mp4', 'Normal_Videos690_x264.mp4', 'Normal_Videos691_x264.mp4', 'Normal_Videos692_x264.mp4', 'Normal_Videos693_x264.mp4', 'Normal_Videos694_x264.mp4', 'Normal_Videos695_x264.mp4', 'Normal_Videos697_x264.mp4', 'Normal_Videos698_x264.mp4', 'Normal_Videos699_x264.mp4', 'Normal_Videos700_x264.mp4', 'Normal_Videos701_x264.mp4', 'Normal_Videos703_x264.mp4', 'Normal_Videos705_x264.mp4', 'Normal_Videos706_x264.mp4', 'Normal_Videos707_x264.mp4', 'Normal_Videos708_x264.mp4', 'Normal_Videos709_x264.mp4', 'Normal_Videos711_x264.mp4', 'Normal_Videos712_x264.mp4', 'Normal_Videos713_x264.mp4', 'Normal_Videos714_x264.mp4', 'Normal_Videos715_x264.mp4', 'Normal_Videos716_x264.mp4', 'Normal_Videos718_x264.mp4', 'Normal_Videos719_x264.mp4', 'Normal_Videos720_x264.mp4', 'Normal_Videos721_x264.mp4', 'Normal_Videos723_x264.mp4', 'Normal_Videos724_x264.mp4', 'Normal_Videos726_x264.mp4', 'Normal_Videos727_x264.mp4', 'Normal_Videos728_x264.mp4', 'Normal_Videos729_x264.mp4', 'Normal_Videos730_x264.mp4', 'Normal_Videos731_x264.mp4', 'Normal_Videos732_x264.mp4', 'Normal_Videos733_x264.mp4', 'Normal_Videos734_x264.mp4', 'Normal_Videos735_x264.mp4', 'Normal_Videos736_x264.mp4', 'Normal_Videos737_x264.mp4', 'Normal_Videos738_x264.mp4', 'Normal_Videos739_x264.mp4', 'Normal_Videos740_x264.mp4', 'Normal_Videos741_x264.mp4', 'Normal_Videos742_x264.mp4', 'Normal_Videos743_x264.mp4', 'Normal_Videos744_x264.mp4', 'Normal_Videos746_x264.mp4', 'Normal_Videos747_x264.mp4', 'Normal_Videos748_x264.mp4', 'Normal_Videos749_x264.mp4', 'Normal_Videos750_x264.mp4', 'Normal_Videos751_x264.mp4', 'Normal_Videos752_x264.mp4', 'Normal_Videos753_x264.mp4', 'Normal_Videos754_x264.mp4', 'Normal_Videos755_x264.mp4', 'Normal_Videos756_x264.mp4', 'Normal_Videos757_x264.mp4', 'Normal_Videos759_x264.mp4', 'Normal_Videos760_x264.mp4', 'Normal_Videos761_x264.mp4', 'Normal_Videos762_x264.mp4', 'Normal_Videos763_x264.mp4', 'Normal_Videos764_x264.mp4', 'Normal_Videos765_x264.mp4', 'Normal_Videos766_x264.mp4', 'Normal_Videos767_x264.mp4', 'Normal_Videos768_x264.mp4', 'Normal_Videos769_x264.mp4', 'Normal_Videos770_x264.mp4', 'Normal_Videos771_x264.mp4', 'Normal_Videos772_x264.mp4', 'Normal_Videos773_x264.mp4', 'Normal_Videos774_x264.mp4', 'Normal_Videos775_x264.mp4', 'Normal_Videos776_x264.mp4', 'Normal_Videos777_x264.mp4', 'Normal_Videos779_x264.mp4', 'Normal_Videos784_x264.mp4', 'Normal_Videos785_x264.mp4', 'Normal_Videos786_x264.mp4', 'Normal_Videos787_x264.mp4', 'Normal_Videos788_x264.mp4', 'Normal_Videos789_x264.mp4', 'Normal_Videos790_x264.mp4', 'Normal_Videos791_x264.mp4', 'Normal_Videos792_x264.mp4', 'Normal_Videos793_x264.mp4', 'Normal_Videos794_x264.mp4', 'Normal_Videos795_x264.mp4', 'Normal_Videos796_x264.mp4', 'Normal_Videos797_x264.mp4', 'Normal_Videos799_x264.mp4', 'Normal_Videos800_x264.mp4', 'Normal_Videos802_x264.mp4', 'Normal_Videos803_x264.mp4', 'Normal_Videos804_x264.mp4', 'Normal_Videos805_x264.mp4', 'Normal_Videos806_x264.mp4', 'Normal_Videos807_x264.mp4', 'Normal_Videos808_x264.mp4', 'Normal_Videos809_x264.mp4', 'Normal_Videos810_x264.mp4', 'Normal_Videos811_x264.mp4', 'Normal_Videos812_x264.mp4', 'Normal_Videos813_x264.mp4', 'Normal_Videos814_x264.mp4', 'Normal_Videos815_x264.mp4', 'Normal_Videos816_x264.mp4', 'Normal_Videos817_x264.mp4', 'Normal_Videos818_x264.mp4', 'Normal_Videos819_x264.mp4', 'Normal_Videos820_x264.mp4', 'Normal_Videos821_x264.mp4', 'Normal_Videos822_x264.mp4', 'Normal_Videos823_x264.mp4', 'Normal_Videos824_x264.mp4', 'Normal_Videos825_x264.mp4', 'Normal_Videos826_x264.mp4', 'Normal_Videos827_x264.mp4', 'Normal_Videos829_x264.mp4', 'Normal_Videos830_x264.mp4', 'Normal_Videos832_x264.mp4', 'Normal_Videos833_x264.mp4', 'Normal_Videos834_x264.mp4', 'Normal_Videos835_x264.mp4', 'Normal_Videos836_x264.mp4', 'Normal_Videos837_x264.mp4', 'Normal_Videos838_x264.mp4', 'Normal_Videos839_x264.mp4', 'Normal_Videos840_x264.mp4', 'Normal_Videos841_x264.mp4', 'Normal_Videos842_x264.mp4', 'Normal_Videos843_x264.mp4', 'Normal_Videos844_x264.mp4', 'Normal_Videos845_x264.mp4', 'Normal_Videos846_x264.mp4', 'Normal_Videos847_x264.mp4', 'Normal_Videos848_x264.mp4', 'Normal_Videos849_x264.mp4', 'Normal_Videos850_x264.mp4', 'Normal_Videos851_x264.mp4', 'Normal_Videos852_x264.mp4', 'Normal_Videos853_x264.mp4', 'Normal_Videos854_x264.mp4', 'Normal_Videos855_x264.mp4', 'Normal_Videos856_x264.mp4', 'Normal_Videos857_x264.mp4', 'Normal_Videos858_x264.mp4', 'Normal_Videos859_x264.mp4', 'Normal_Videos860_x264.mp4', 'Normal_Videos861_x264.mp4', 'Normal_Videos862_x264.mp4', 'Normal_Videos863_x264.mp4', 'Normal_Videos864_x264.mp4', 'Normal_Videos865_x264.mp4', 'Normal_Videos916_x264.mp4', 'Normal_Videos917_x264.mp4', 'Normal_Videos918_x264.mp4', 'Normal_Videos919_x264.mp4', 'Normal_Videos920_x264.mp4', 'Normal_Videos921_x264.mp4', 'Normal_Videos922_x264.mp4', 'Normal_Videos942_x264.mp4', 'Normal_Videos945_x264.mp4', 'Normal_Videos946_x264.mp4', 'Normal_Videos947_x264.mp4', 'Normal_Videos948_x264.mp4', 'Normal_Videos949_x264.mp4', 'Normal_Videos950_x264.mp4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for directory_path in DATA_SOURCE.items():\n",
    "# 경로가 존재하는지 확인합니다.\n",
    "    if os.path.exists(directory_path[1]):\n",
    "        print(\"경로가 존재합니다.\")\n",
    "        # 디렉터리 내의 파일 목록을 출력합니다.\n",
    "        print(\"디렉터리 내용:\", os.listdir(directory_path[1]))\n",
    "    else:\n",
    "        print(\"지정된 경로를 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d06374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.338518Z",
     "iopub.status.busy": "2024-03-18T13:10:51.338184Z",
     "iopub.status.idle": "2024-03-18T13:10:51.352085Z",
     "shell.execute_reply": "2024-03-18T13:10:51.351221Z"
    },
    "papermill": {
     "duration": 0.025363,
     "end_time": "2024-03-18T13:10:51.353938",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.328575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrimeDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      2\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, __train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class CrimeDataset(Dataset):\n",
    "    train = True\n",
    "    def __init__(self, __train=True):\n",
    "        train = __train\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        self._frame_interval = frame_interval\n",
    "        \n",
    "        print(f\"Loading {'train' if train else 'test' } dataset...\")\n",
    "\n",
    "        if train:\n",
    "            for label, data_path in DATA_SOURCE.items():\n",
    "                print(f\"Loading Label {label}...\")\n",
    "            \n",
    "                for file in tqdm(os.listdir(data_path)):\n",
    "                    if file.endswith(\".mp4\") and file in train_dic:\n",
    "                        path = os.path.join(data_path, file)\n",
    "                        data, labels = self._parse_file(path, label)\n",
    "                        self._data.extend(data)\n",
    "                        self._labels.extend(labels)\n",
    "        else:\n",
    "            for label, data_path in DATA_SOURCE.items():\n",
    "                print(f\"Loading Label {label}...\")\n",
    "            \n",
    "                for file in tqdm(os.listdir(data_path)):\n",
    "                    if file.endswith(\".mp4\") and file in test_dic:\n",
    "                        path = os.path.join(data_path, file)\n",
    "                        data, labels = self._parse_file(path, label)\n",
    "                        self._data.extend(data)\n",
    "                        self._labels.extend(labels)\n",
    "                    \n",
    "        print(f\"Finished loading {'train' if train else 'test' } dataset... Loaded  {len(self._data)} images.\")\n",
    "    \n",
    "    def _parse_file(self, __path, __label):\n",
    "        if not os.path.exists(__path):\n",
    "            return [], []\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        frame_count = 0\n",
    "        cap = cv2.VideoCapture(__path)\n",
    "        success, image = cap.read()\n",
    "        while success:\n",
    "            try:\n",
    "                if True:\n",
    "                    \n",
    "                    if train:\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "                        Y, U, V = cv2.split(image)\n",
    "                        image = transformer(Y)\n",
    "                        data.append(image)\n",
    "                        labels.append(__label) \n",
    "                                \n",
    "                    else:\n",
    "                        data.append(image)\n",
    "                        filepath_list = __path.split('\\\\')\n",
    "                        event_name, start_frame1, end_frame1, start_frame2, end_frame2 = test_dic[filepath_list[-1]]\n",
    "                        if (frame_count >= start_frame1 and frame_count <= end_frame1) or (frame_count >= start_frame2 and frame_count <= end_frame2):\n",
    "                            labels.append(__label)\n",
    "                        else:\n",
    "                            labels.append(\"Normal\")\n",
    "                    \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {__path}: {e}\")\n",
    "            \n",
    "            count = 0\n",
    "            while success and count < self._frame_interval:\n",
    "                success, image = cap.read()\n",
    "                count += 1 \n",
    "        return data, labels\n",
    "        \n",
    "    \n",
    "    def label_str2id(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels).index(__label)\n",
    "\n",
    "    def label_id2str(self, __label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels)[__label]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, __idx):\n",
    "        data = self._data[__idx]\n",
    "        label = self._labels[__idx]\n",
    "        return data, torch.tensor([self.label_str2id(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5eab9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:10:51.371922Z",
     "iopub.status.busy": "2024-03-18T13:10:51.371655Z",
     "iopub.status.idle": "2024-03-18T13:21:45.090780Z",
     "shell.execute_reply": "2024-03-18T13:21:45.089667Z"
    },
    "papermill": {
     "duration": 653.730762,
     "end_time": "2024-03-18T13:21:45.093180",
     "exception": false,
     "start_time": "2024-03-18T13:10:51.362418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CrimeDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCrimeDataset\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CrimeDataset(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CrimeDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "train_dataset = CrimeDataset(True)\n",
    "train = False\n",
    "test_dataset = CrimeDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63356b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:45.223016Z",
     "iopub.status.busy": "2024-03-18T13:21:45.222687Z",
     "iopub.status.idle": "2024-03-18T13:21:46.551732Z",
     "shell.execute_reply": "2024-03-18T13:21:46.550759Z"
    },
    "papermill": {
     "duration": 1.421088,
     "end_time": "2024-03-18T13:21:46.557862",
     "exception": false,
     "start_time": "2024-03-18T13:21:45.136774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_randomly_form_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_randomly_form_dataset\u001b[49m(train_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_randomly_form_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "plot_randomly_form_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3157940e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:46.663279Z",
     "iopub.status.busy": "2024-03-18T13:21:46.662934Z",
     "iopub.status.idle": "2024-03-18T13:21:46.667944Z",
     "shell.execute_reply": "2024-03-18T13:21:46.667044Z"
    },
    "papermill": {
     "duration": 0.060875,
     "end_time": "2024-03-18T13:21:46.669971",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.609096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dcd2d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_sequence_form_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_sequence_form_dataset\u001b[49m(train_dataset, \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_sequence_form_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_sequence_form_dataset(train_dataset, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf2e37",
   "metadata": {
    "papermill": {
     "duration": 0.049683,
     "end_time": "2024-03-18T13:21:46.769625",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.719942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b31ec59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:46.871572Z",
     "iopub.status.busy": "2024-03-18T13:21:46.870902Z",
     "iopub.status.idle": "2024-03-18T13:21:46.881917Z",
     "shell.execute_reply": "2024-03-18T13:21:46.881046Z"
    },
    "papermill": {
     "duration": 0.064385,
     "end_time": "2024-03-18T13:21:46.884020",
     "exception": false,
     "start_time": "2024-03-18T13:21:46.819635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_divisible(\n",
    "        value: float,\n",
    "        divisor: int,\n",
    "        min_value: Optional[float] = None,\n",
    "        round_down_protect: bool = True,\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    This function is copied from here \n",
    "    \"https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_layers.py\"\n",
    "    \n",
    "    This is to ensure that all layers have channels that are divisible by 8.\n",
    "\n",
    "    Args:\n",
    "        value: A `float` of original value.\n",
    "        divisor: An `int` of the divisor that need to be checked upon.\n",
    "        min_value: A `float` of  minimum value threshold.\n",
    "        round_down_protect: A `bool` indicating whether round down more than 10%\n",
    "        will be allowed.\n",
    "\n",
    "    Returns:\n",
    "        The adjusted value in `int` that is divisible against divisor.\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if round_down_protect and new_value < 0.9 * value:\n",
    "        new_value += divisor\n",
    "    return int(new_value)\n",
    "def conv_2d(inp, oup, kernel_size=3, stride=1, groups=1, bias=False, norm=True, act=True):\n",
    "    conv = nn.Sequential()\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    conv.add_module('conv', nn.Conv2d(inp, oup, kernel_size, stride, padding, bias=bias, groups=groups))\n",
    "    if norm:\n",
    "        conv.add_module('BatchNorm2d', nn.BatchNorm2d(oup))\n",
    "    if act:\n",
    "        conv.add_module('Activation', nn.ReLU6())\n",
    "    return conv\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, act=False, squeeze_excitation=False):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.block = nn.Sequential()\n",
    "        if expand_ratio != 1:\n",
    "            self.block.add_module('exp_1x1', conv_2d(inp, hidden_dim, kernel_size=3, stride=stride))\n",
    "        if squeeze_excitation:\n",
    "            self.block.add_module('conv_3x3', conv_2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, groups=hidden_dim))\n",
    "        self.block.add_module('red_1x1', conv_2d(hidden_dim, oup, kernel_size=1, stride=1, act=act))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)\n",
    "\n",
    "class UniversalInvertedBottleneckBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "            inp, \n",
    "            oup, \n",
    "            start_dw_kernel_size, \n",
    "            middle_dw_kernel_size, \n",
    "            middle_dw_downsample,\n",
    "            stride,\n",
    "            expand_ratio\n",
    "        ):\n",
    "        \"\"\"An inverted bottleneck block with optional depthwises.\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Starting depthwise conv.\n",
    "        self.start_dw_kernel_size = start_dw_kernel_size\n",
    "        if self.start_dw_kernel_size:            \n",
    "            stride_ = stride if not middle_dw_downsample else 1\n",
    "            self._start_dw_ = conv_2d(inp, inp, kernel_size=start_dw_kernel_size, stride=stride_, groups=inp, act=False)\n",
    "        # Expansion with 1x1 convs.\n",
    "        expand_filters = make_divisible(inp * expand_ratio, 8)\n",
    "        self._expand_conv = conv_2d(inp, expand_filters, kernel_size=1)\n",
    "        # Middle depthwise conv.\n",
    "        self.middle_dw_kernel_size = middle_dw_kernel_size\n",
    "        if self.middle_dw_kernel_size:\n",
    "            stride_ = stride if middle_dw_downsample else 1\n",
    "            self._middle_dw = conv_2d(expand_filters, expand_filters, kernel_size=middle_dw_kernel_size, stride=stride_, groups=expand_filters)\n",
    "        # Projection with 1x1 convs.\n",
    "        self._proj_conv = conv_2d(expand_filters, oup, kernel_size=1, stride=1, act=False)\n",
    "        \n",
    "        # Ending depthwise conv.\n",
    "        # this not used\n",
    "        # _end_dw_kernel_size = 0\n",
    "        # self._end_dw = conv_2d(oup, oup, kernel_size=_end_dw_kernel_size, stride=stride, groups=inp, act=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.start_dw_kernel_size:\n",
    "            x = self._start_dw_(x)\n",
    "            # print(\"_start_dw_\", x.shape)\n",
    "        x = self._expand_conv(x)\n",
    "        # print(\"_expand_conv\", x.shape)\n",
    "        if self.middle_dw_kernel_size:\n",
    "            x = self._middle_dw(x)\n",
    "            # print(\"_middle_dw\", x.shape)\n",
    "        x = self._proj_conv(x)\n",
    "        # print(\"_proj_conv\", x.shape)\n",
    "        return x\n",
    "\n",
    "class MultiQueryAttentionLayerWithDownSampling(nn.Module):\n",
    "    def __init__(self, inp, num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides, dw_kernel_size=3, dropout=0.0):\n",
    "        \"\"\"Multi Query Attention with spatial downsampling.\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "\n",
    "        3 parameters are introduced for the spatial downsampling:\n",
    "        1. kv_strides: downsampling factor on Key and Values only.\n",
    "        2. query_h_strides: vertical strides on Query only.\n",
    "        3. query_w_strides: horizontal strides on Query only.\n",
    "\n",
    "        This is an optimized version.\n",
    "        1. Projections in Attention is explict written out as 1x1 Conv2D.\n",
    "        2. Additional reshapes are introduced to bring a up to 3x speed up.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.query_h_strides = query_h_strides\n",
    "        self.query_w_strides = query_w_strides\n",
    "        self.kv_strides = kv_strides\n",
    "        self.dw_kernel_size = dw_kernel_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = key_dim // num_heads\n",
    "\n",
    "        if self.query_h_strides > 1 or self.query_w_strides > 1:\n",
    "            self._query_downsampling_norm = nn.BatchNorm2d(inp)\n",
    "        self._query_proj = conv_2d(inp, num_heads*key_dim, 1, 1, norm=False, act=False)\n",
    "        \n",
    "        if self.kv_strides > 1:\n",
    "            self._key_dw_conv = conv_2d(inp, inp, dw_kernel_size, kv_strides, groups=inp, norm=True, act=False)\n",
    "            self._value_dw_conv = conv_2d(inp, inp, dw_kernel_size, kv_strides, groups=inp, norm=True, act=False)\n",
    "        self._key_proj = conv_2d(inp, key_dim, 1, 1, norm=False, act=False)\n",
    "        self._value_proj = conv_2d(inp, key_dim, 1, 1, norm=False, act=False)\n",
    "\n",
    "        self._output_proj = conv_2d(num_heads*key_dim, inp, 1, 1, norm=False, act=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _, _ = x.size()\n",
    "        if self.query_h_strides > 1 or self.query_w_strides > 1:\n",
    "            q = F.avg_pool2d(self.query_h_stride, self.query_w_stride)\n",
    "            q = self._query_downsampling_norm(q)\n",
    "            q = self._query_proj(q)\n",
    "        else:\n",
    "            q = self._query_proj(x)\n",
    "        px = q.size(2)\n",
    "        q = q.view(batch_size, self.num_heads, -1, self.key_dim) # [batch_size, num_heads, seq_length, key_dim]\n",
    "\n",
    "        if self.kv_strides > 1:\n",
    "            k = self._key_dw_conv(x)\n",
    "            k = self._key_proj(k)\n",
    "            v = self._value_dw_conv(x)\n",
    "            v = self._value_proj(v)          \n",
    "        else:\n",
    "            k = self._key_proj(x)\n",
    "            v = self._value_proj(x)\n",
    "        k = k.view(batch_size, 1, self.key_dim, -1) # [batch_size, 1, key_dim, seq_length]\n",
    "        v = v.view(batch_size, 1, -1, self.key_dim) # [batch_size, 1, seq_length, key_dim]\n",
    "\n",
    "        # calculate attn score\n",
    "        attn_score = torch.matmul(q, k) / (self.head_dim ** 0.5)\n",
    "        attn_score = self.dropout(attn_score)\n",
    "        attn_score = F.softmax(attn_score, dim=-1)\n",
    "\n",
    "        context = torch.matmul(attn_score, v)\n",
    "        context = context.view(batch_size, self.num_heads * self.key_dim, px, px)\n",
    "        output = self._output_proj(context)\n",
    "        return output\n",
    "\n",
    "class MNV4LayerScale(nn.Module):\n",
    "    def __init__(self, inp, init_value):\n",
    "        \"\"\"LayerScale as introduced in CaiT: https://arxiv.org/abs/2103.17239\n",
    "        Referenced from here https://github.com/tensorflow/models/blob/master/official/vision/modeling/layers/nn_blocks.py\n",
    "        \n",
    "        As used in MobileNetV4.\n",
    "\n",
    "        Attributes:\n",
    "            init_value (float): value to initialize the diagonal matrix of LayerScale.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.init_value = init_value\n",
    "        self._gamma = nn.Parameter(self.init_value * torch.ones(inp, 1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self._gamma\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            inp,\n",
    "            num_heads, \n",
    "            key_dim,  \n",
    "            value_dim, \n",
    "            query_h_strides, \n",
    "            query_w_strides, \n",
    "            kv_strides,\n",
    "            use_layer_scale,\n",
    "            use_multi_query, \n",
    "            use_residual = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.query_h_strides = query_h_strides\n",
    "        self.query_w_strides = query_w_strides\n",
    "        self.kv_strides = kv_strides\n",
    "        self.use_layer_scale = use_layer_scale\n",
    "        self.use_multi_query = use_multi_query\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        self._input_norm = nn.BatchNorm2d(inp)\n",
    "        if self.use_multi_query:\n",
    "            self.multi_query_attention = MultiQueryAttentionLayerWithDownSampling(\n",
    "                inp, num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides\n",
    "            )\n",
    "        else:\n",
    "            self.multi_head_attention = nn.MultiheadAttention(inp, num_heads, kdim=key_dim)\n",
    "        \n",
    "        if self.use_layer_scale:\n",
    "            self.layer_scale_init_value = 1e-5\n",
    "            self.layer_scale = MNV4LayerScale(inp, self.layer_scale_init_value) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Not using CPE, skipped\n",
    "        # input norm\n",
    "        shortcut = x\n",
    "        x = self._input_norm(x)\n",
    "        # multi query\n",
    "        if self.use_multi_query:\n",
    "            x = self.multi_query_attention(x)\n",
    "        else:\n",
    "            x = self.multi_head_attention(x, x)\n",
    "        # layer scale\n",
    "        if self.use_layer_scale:\n",
    "            x = self.layer_scale(x)\n",
    "        # use residual\n",
    "        if self.use_residual:\n",
    "            x = x + shortcut\n",
    "        return x\n",
    "\n",
    "def build_blocks(layer_spec):\n",
    "    if not layer_spec.get('block_name'):\n",
    "        return nn.Sequential()\n",
    "    block_names = layer_spec['block_name']\n",
    "    layers = nn.Sequential()\n",
    "    if block_names == \"convbn\":\n",
    "        schema_ = ['inp', 'oup', 'kernel_size', 'stride']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            layers.add_module(f\"convbn_{i}\", conv_2d(**args))\n",
    "    elif block_names == \"uib\":\n",
    "        schema_ =  ['inp', 'oup', 'start_dw_kernel_size', 'middle_dw_kernel_size', 'middle_dw_downsample', 'stride', 'expand_ratio', 'mhsa']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            mhsa = args.pop(\"mhsa\") if \"mhsa\" in args else 0\n",
    "            layers.add_module(f\"uib_{i}\", UniversalInvertedBottleneckBlock(**args))\n",
    "            if mhsa:\n",
    "                mhsa_schema_ = [\n",
    "                    \"inp\", \"num_heads\", \"key_dim\", \"value_dim\", \"query_h_strides\", \"query_w_strides\", \"kv_strides\", \n",
    "                    \"use_layer_scale\", \"use_multi_query\", \"use_residual\"\n",
    "                ]\n",
    "                args = dict(zip(mhsa_schema_, [args['oup']] + (mhsa)))\n",
    "                layers.add_module(f\"mhsa_{i}\", MultiHeadSelfAttentionBlock(**args))\n",
    "    elif block_names == \"fused_ib\":\n",
    "        schema_ = ['inp', 'oup', 'stride', 'expand_ratio', 'act']\n",
    "        for i in range(layer_spec['num_blocks']):\n",
    "            args = dict(zip(schema_, layer_spec['block_specs'][i]))\n",
    "            layers.add_module(f\"fused_ib_{i}\", InvertedResidual(**args))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return layers\n",
    "\n",
    "\n",
    "class MobileNetV4(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        # MobileNetV4ConvSmall  MobileNetV4ConvMedium  MobileNetV4ConvLarge\n",
    "        # MobileNetV4HybridMedium  MobileNetV4HybridLarge\n",
    "        \"\"\"Params to initiate MobilenNetV4\n",
    "        Args:\n",
    "            model : support 5 types of models as indicated in \n",
    "            \"https://github.com/tensorflow/models/blob/master/official/vision/modeling/backbones/mobilenet.py\"        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert model in MODEL_SPECS.keys()\n",
    "        self.model = model\n",
    "        self.spec = MODEL_SPECS[self.model]\n",
    "       \n",
    "        # conv0\n",
    "        self.conv0 = build_blocks(self.spec['conv0'])\n",
    "        # layer1\n",
    "        self.layer1 = build_blocks(self.spec['layer1'])\n",
    "        # layer2\n",
    "        self.layer2 = build_blocks(self.spec['layer2'])\n",
    "        # layer3\n",
    "        self.layer3 = build_blocks(self.spec['layer3'])\n",
    "        # layer4\n",
    "        self.layer4 = build_blocks(self.spec['layer4'])\n",
    "        # layer5   \n",
    "        self.layer5 = build_blocks(self.spec['layer5'])       \n",
    "               \n",
    "    def forward(self, x):\n",
    "        x0 = self.conv0(x)\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        x5 = nn.functional.adaptive_avg_pool2d(x5, 1 )\n",
    "        return [x1, x2, x3, x4, x5]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "faf8c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MNV4ConvSmall_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [32, 32, 3, 2],\n",
    "            [32, 32, 1, 1]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [32, 96, 3, 2],\n",
    "            [96, 64, 1, 1]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 6,\n",
    "        \"block_specs\": [\n",
    "            [64, 96, 5, 5, True, 2, 3],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 0, 3, True, 1, 2],\n",
    "            [96, 96, 3, 0, True, 1, 4],\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 6,\n",
    "        \"block_specs\": [\n",
    "            [96,  128, 3, 3, True, 2, 6],\n",
    "            [128, 128, 5, 5, True, 1, 4],\n",
    "            [128, 128, 0, 5, True, 1, 4],\n",
    "            [128, 128, 0, 5, True, 1, 3],\n",
    "            [128, 128, 0, 3, True, 1, 4],\n",
    "            [128, 128, 0, 3, True, 1, 4],\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [128, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4ConvMedium_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [32, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 80, 3, 5, True, 2, 4],\n",
    "            [80, 80, 3, 3, True, 1, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 8,\n",
    "        \"block_specs\": [\n",
    "            [80,  160, 3, 5, True, 2, 6],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 5, True, 1, 4],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 0, True, 1, 4],\n",
    "            [160, 160, 0, 0, True, 1, 2],\n",
    "            [160, 160, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [160, 256, 5, 5, True, 2, 6],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 3, 0, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 2],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 4],\n",
    "            [256, 256, 5, 0, True, 1, 2]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [256, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4ConvLarge_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [1, 24, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [24, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 96, 3, 5, True, 2, 4],\n",
    "            [96, 96, 3, 3, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [96,  192, 3, 5, True, 2, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 5, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 13,\n",
    "        \"block_specs\": [\n",
    "            [192, 512, 5, 5, True, 2, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [512, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def mhsa(num_heads, key_dim, value_dim, px):\n",
    "    if px == 24:\n",
    "        kv_strides = 2\n",
    "    elif px == 12:\n",
    "        kv_strides = 1\n",
    "    query_h_strides = 1 \n",
    "    query_w_strides = 1 \n",
    "    use_layer_scale = True \n",
    "    use_multi_query = True\n",
    "    use_residual = True\n",
    "    return [\n",
    "        num_heads, key_dim, value_dim, query_h_strides, query_w_strides, kv_strides, \n",
    "        use_layer_scale, use_multi_query, use_residual\n",
    "    ]\n",
    "\n",
    "MNV4HybridConvMedium_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [3, 32, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [32, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 80, 3, 5, True, 2, 4],\n",
    "            [80, 80, 3, 3, True, 1, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 8,\n",
    "        \"block_specs\": [\n",
    "            [80,  160, 3, 5, True, 2, 6],\n",
    "            [160, 160, 0, 0, True, 1, 2],\n",
    "            [160, 160, 3, 3, True, 1, 4],\n",
    "            [160, 160, 3, 5, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 3, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 0, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 3, True, 1, 4, mhsa(4, 64, 64, 24)],\n",
    "            [160, 160, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 12,\n",
    "        \"block_specs\": [\n",
    "            [160, 256, 5, 5, True, 2, 6],\n",
    "            [256, 256, 5, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 3, 5, True, 1, 4],\n",
    "            [256, 256, 0, 0, True, 1, 2],\n",
    "            [256, 256, 3, 5, True, 1, 2],\n",
    "            [256, 256, 0, 0, True, 1, 2],\n",
    "            [256, 256, 0, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 3, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 5, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 0, True, 1, 4, mhsa(4, 64, 64, 12)],\n",
    "            [256, 256, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [256, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MNV4HybridConvLarge_BLOCK_SPECS = {\n",
    "    \"conv0\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [3, 24, 3, 2]\n",
    "        ]\n",
    "    },\n",
    "    \"layer1\": {\n",
    "        \"block_name\": \"fused_ib\",\n",
    "        \"num_blocks\": 1,\n",
    "        \"block_specs\": [\n",
    "            [24, 48, 2, 4.0, True]\n",
    "        ]\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [48, 96, 3, 5, True, 2, 4],\n",
    "            [96, 96, 3, 3, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer3\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 11,\n",
    "        \"block_specs\": [\n",
    "            [96,  192, 3, 5, True, 2, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 3, True, 1, 4],\n",
    "            [192, 192, 3, 5, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 5, 3, True, 1, 4, mhsa(8, 48, 48, 24)],\n",
    "            [192, 192, 3, 0, True, 1, 4]\n",
    "        ]\n",
    "    },\n",
    "    \"layer4\": {\n",
    "        \"block_name\": \"uib\",\n",
    "        \"num_blocks\": 14,\n",
    "        \"block_specs\": [\n",
    "            [192, 512, 5, 5, True, 2, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 0, True, 1, 4],\n",
    "            [512, 512, 5, 3, True, 1, 4],\n",
    "            [512, 512, 5, 5, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4, mhsa(8, 64, 64, 12)],\n",
    "            [512, 512, 5, 0, True, 1, 4]\n",
    "        ]\n",
    "    },  \n",
    "    \"layer5\": {\n",
    "        \"block_name\": \"convbn\",\n",
    "        \"num_blocks\": 2,\n",
    "        \"block_specs\": [\n",
    "            [512, 960, 1, 1],\n",
    "            [960, 1280, 1, 1]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    \"MobileNetV4ConvSmall\": MNV4ConvSmall_BLOCK_SPECS,\n",
    "    \"MobileNetV4ConvMedium\": MNV4ConvMedium_BLOCK_SPECS,\n",
    "    \"MobileNetV4ConvLarge\": MNV4ConvLarge_BLOCK_SPECS,\n",
    "    \"MobileNetV4HybridMedium\": MNV4HybridConvMedium_BLOCK_SPECS,\n",
    "    \"MobileNetV4HybridLarge\": MNV4HybridConvLarge_BLOCK_SPECS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d75303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Support ['MobileNetV4ConvSmall', 'MobileNetV4ConvMedium', 'MobileNetV4ConvLarge']\n",
    "# # Also supported ['MobileNetV4HybridMedium', 'MobileNetV4HybridLarge']\n",
    "# model = MobileNetV4(\"MobileNetV4ConvSmall\")\n",
    "\n",
    "# # Check the trainable params\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "# # Check the model's output shape\n",
    "# print(\"Check output shape ...\")\n",
    "# x = torch.rand(4, 1, 8, 64, 64)\n",
    "# y = model(x)\n",
    "# for i in y:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3789d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 모델 정의\n",
    "class FCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.fc = nn.Linear(1280, 14)  # 예시로 출력 뉴런 수를 10개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dee7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn = MobileNetV4(\"MobileNetV4ConvSmall\")\n",
    "        self.fc = FCModel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x[4]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3921927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "model = CombinedModel()\n",
    "x = torch.rand(4, 1, 64, 64)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc3dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8279d20",
   "metadata": {
    "papermill": {
     "duration": 0.058478,
     "end_time": "2024-03-18T13:21:48.387320",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.328842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78aa6ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:48.502470Z",
     "iopub.status.busy": "2024-03-18T13:21:48.501607Z",
     "iopub.status.idle": "2024-03-18T13:21:48.506944Z",
     "shell.execute_reply": "2024-03-18T13:21:48.506075Z"
    },
    "papermill": {
     "duration": 0.064273,
     "end_time": "2024-03-18T13:21:48.508993",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.444720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03347ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:48.613921Z",
     "iopub.status.busy": "2024-03-18T13:21:48.613620Z",
     "iopub.status.idle": "2024-03-18T13:21:48.993707Z",
     "shell.execute_reply": "2024-03-18T13:21:48.992772Z"
    },
    "papermill": {
     "duration": 0.433887,
     "end_time": "2024-03-18T13:21:48.995895",
     "exception": false,
     "start_time": "2024-03-18T13:21:48.562008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CombinedModel(\n",
       "    (cnn): MobileNetV4(\n",
       "      (conv0): Sequential(\n",
       "        (convbn_0): Sequential(\n",
       "          (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "      )\n",
       "      (layer1): Sequential(\n",
       "        (convbn_0): Sequential(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "        (convbn_1): Sequential(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (convbn_0): Sequential(\n",
       "          (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "        (convbn_1): Sequential(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (uib_0): UniversalInvertedBottleneckBlock(\n",
       "          (_start_dw_): Sequential(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_1): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_2): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_3): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_4): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_5): UniversalInvertedBottleneckBlock(\n",
       "          (_start_dw_): Sequential(\n",
       "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (uib_0): UniversalInvertedBottleneckBlock(\n",
       "          (_start_dw_): Sequential(\n",
       "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_1): UniversalInvertedBottleneckBlock(\n",
       "          (_start_dw_): Sequential(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_2): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_3): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_4): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (uib_5): UniversalInvertedBottleneckBlock(\n",
       "          (_expand_conv): Sequential(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_middle_dw): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (Activation): ReLU6()\n",
       "          )\n",
       "          (_proj_conv): Sequential(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (BatchNorm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer5): Sequential(\n",
       "        (convbn_0): Sequential(\n",
       "          (conv): Conv2d(128, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "        (convbn_1): Sequential(\n",
       "          (conv): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (BatchNorm2d): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Activation): ReLU6()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): FCModel(\n",
       "      (fc): Linear(in_features=1280, out_features=14, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ed03c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.103520Z",
     "iopub.status.busy": "2024-03-18T13:21:49.103183Z",
     "iopub.status.idle": "2024-03-18T13:21:49.107298Z",
     "shell.execute_reply": "2024-03-18T13:21:49.106444Z"
    },
    "papermill": {
     "duration": 0.057751,
     "end_time": "2024-03-18T13:21:49.109369",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.051618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b489c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.211609Z",
     "iopub.status.busy": "2024-03-18T13:21:49.210820Z",
     "iopub.status.idle": "2024-03-18T13:21:49.215238Z",
     "shell.execute_reply": "2024-03-18T13:21:49.214354Z"
    },
    "papermill": {
     "duration": 0.058531,
     "end_time": "2024-03-18T13:21:49.217169",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.158638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "412f048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  29124 KiB |  29124 KiB |  29131 KiB |   6656 B   |\\n|       from large pool |  17792 KiB |  17792 KiB |  17792 KiB |      0 B   |\\n|       from small pool |  11332 KiB |  11332 KiB |  11339 KiB |   6656 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  29124 KiB |  29124 KiB |  29131 KiB |   6656 B   |\\n|       from large pool |  17792 KiB |  17792 KiB |  17792 KiB |      0 B   |\\n|       from small pool |  11332 KiB |  11332 KiB |  11339 KiB |   6656 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  29025 KiB |  29025 KiB |  29031 KiB |   6272 B   |\\n|       from large pool |  17792 KiB |  17792 KiB |  17792 KiB |      0 B   |\\n|       from small pool |  11233 KiB |  11233 KiB |  11239 KiB |   6272 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  32768 KiB |  32768 KiB |  32768 KiB |      0 B   |\\n|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |\\n|       from small pool |  12288 KiB |  12288 KiB |  12288 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   3643 KiB |  17694 KiB |  25925 KiB |  22281 KiB |\\n|       from large pool |   2688 KiB |  15680 KiB |  15680 KiB |  12992 KiB |\\n|       from small pool |    955 KiB |   2046 KiB |  10245 KiB |   9289 KiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     559    |     559    |     560    |       1    |\\n|       from large pool |       3    |       3    |       3    |       0    |\\n|       from small pool |     556    |     556    |     557    |       1    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     559    |     559    |     560    |       1    |\\n|       from large pool |       3    |       3    |       3    |       0    |\\n|       from small pool |     556    |     556    |     557    |       1    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       7    |       7    |       7    |       0    |\\n|       from large pool |       1    |       1    |       1    |       0    |\\n|       from small pool |       6    |       6    |       6    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       3    |       4    |       7    |       4    |\\n|       from large pool |       1    |       1    |       1    |       0    |\\n|       from small pool |       2    |       3    |       6    |       4    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df8597c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:21:49.320218Z",
     "iopub.status.busy": "2024-03-18T13:21:49.319329Z",
     "iopub.status.idle": "2024-03-18T13:24:03.470004Z",
     "shell.execute_reply": "2024-03-18T13:24:03.468897Z"
    },
    "papermill": {
     "duration": 134.204958,
     "end_time": "2024-03-18T13:24:03.472064",
     "exception": false,
     "start_time": "2024-03-18T13:21:49.267106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0...\n",
      "Loss: 2.6489648818969727, Batch: 1/440\n",
      "Loss: 1.1063780784606934, Batch: 21/440\n",
      "Loss: 0.7480365037918091, Batch: 41/440\n",
      "Loss: 0.2403491586446762, Batch: 61/440\n",
      "Loss: 0.1578969657421112, Batch: 81/440\n",
      "Loss: 0.3680190145969391, Batch: 101/440\n",
      "Loss: 0.15743491053581238, Batch: 121/440\n",
      "Loss: 0.13959310948848724, Batch: 141/440\n",
      "Loss: 0.24642392992973328, Batch: 161/440\n",
      "Loss: 0.11588810384273529, Batch: 181/440\n",
      "Loss: 0.0747627466917038, Batch: 201/440\n",
      "Loss: 0.22265027463436127, Batch: 221/440\n",
      "Loss: 0.41481491923332214, Batch: 241/440\n",
      "Loss: 0.0855572447180748, Batch: 261/440\n",
      "Loss: 0.015304411761462688, Batch: 281/440\n",
      "Loss: 0.21853283047676086, Batch: 301/440\n",
      "Loss: 0.0991893783211708, Batch: 321/440\n",
      "Loss: 0.018327336758375168, Batch: 341/440\n",
      "Loss: 0.09098103642463684, Batch: 361/440\n",
      "Loss: 0.08602206408977509, Batch: 381/440\n",
      "Loss: 0.03739415854215622, Batch: 401/440\n",
      "Loss: 0.024903586134314537, Batch: 421/440\n",
      "Epoch : 1...440\t\t\n",
      "Loss: 0.2329540252685547, Batch: 1/440\n",
      "Loss: 0.40613678097724915, Batch: 21/440\n",
      "Loss: 0.33816784620285034, Batch: 41/440\n",
      "Loss: 0.2693774700164795, Batch: 61/440\n",
      "Loss: 0.3605519235134125, Batch: 81/440\n",
      "Loss: 0.10570280998945236, Batch: 101/440\n",
      "Loss: 0.05321842059493065, Batch: 121/440\n",
      "Loss: 0.006239510606974363, Batch: 141/440\n",
      "Loss: 0.13463672995567322, Batch: 161/440\n",
      "Loss: 0.05173232778906822, Batch: 181/440\n",
      "Loss: 0.011160334572196007, Batch: 201/440\n",
      "Loss: 0.1348772943019867, Batch: 221/440\n",
      "Loss: 0.03130009397864342, Batch: 241/440\n",
      "Loss: 0.30615943670272827, Batch: 261/440\n",
      "Loss: 0.4132867753505707, Batch: 281/440\n",
      "Loss: 0.15075355768203735, Batch: 301/440\n",
      "Loss: 0.12264399975538254, Batch: 321/440\n",
      "Loss: 0.0236101932823658, Batch: 341/440\n",
      "Loss: 0.11939357966184616, Batch: 361/440\n",
      "Loss: 0.03415275737643242, Batch: 381/440\n",
      "Loss: 0.1048489585518837, Batch: 401/440\n",
      "Loss: 0.03588245436549187, Batch: 421/440\n",
      "Epoch : 2...440\t\t\n",
      "Loss: 0.037423621863126755, Batch: 1/440\n",
      "Loss: 0.1396186351776123, Batch: 21/440\n",
      "Loss: 0.058287061750888824, Batch: 41/440\n",
      "Loss: 0.06834039092063904, Batch: 61/440\n",
      "Loss: 0.027476351708173752, Batch: 81/440\n",
      "Loss: 0.038166336715221405, Batch: 101/440\n",
      "Loss: 0.008866386488080025, Batch: 121/440\n",
      "Loss: 0.027742156758904457, Batch: 141/440\n",
      "Loss: 0.009509917348623276, Batch: 161/440\n",
      "Loss: 0.04783804342150688, Batch: 181/440\n",
      "Loss: 0.12714630365371704, Batch: 201/440\n",
      "Loss: 0.07069838792085648, Batch: 221/440\n",
      "Loss: 0.014940214343369007, Batch: 241/440\n",
      "Loss: 0.0012138866586610675, Batch: 261/440\n",
      "Loss: 0.04872867092490196, Batch: 281/440\n",
      "Loss: 0.02310866303741932, Batch: 301/440\n",
      "Loss: 0.18228505551815033, Batch: 321/440\n",
      "Loss: 0.005092089995741844, Batch: 341/440\n",
      "Loss: 0.0039011090993881226, Batch: 361/440\n",
      "Loss: 0.06509947031736374, Batch: 381/440\n",
      "Loss: 0.008336683735251427, Batch: 401/440\n",
      "Loss: 0.004633020609617233, Batch: 421/440\n",
      "Epoch : 3...440\t\t\n",
      "Loss: 0.1884663850069046, Batch: 1/440\n",
      "Loss: 0.044325053691864014, Batch: 21/440\n",
      "Loss: 0.1438528597354889, Batch: 41/440\n",
      "Loss: 0.040417224168777466, Batch: 61/440\n",
      "Loss: 0.12863077223300934, Batch: 81/440\n",
      "Loss: 0.02288612723350525, Batch: 101/440\n",
      "Loss: 0.009003838524222374, Batch: 121/440\n",
      "Loss: 0.036070648580789566, Batch: 141/440\n",
      "Loss: 0.03043018840253353, Batch: 161/440\n",
      "Loss: 0.04831548035144806, Batch: 181/440\n",
      "Loss: 0.04801042750477791, Batch: 201/440\n",
      "Loss: 0.060679707676172256, Batch: 221/440\n",
      "Loss: 0.1195349246263504, Batch: 241/440\n",
      "Loss: 0.04365567862987518, Batch: 261/440\n",
      "Loss: 0.009287374094128609, Batch: 281/440\n",
      "Loss: 0.11064334213733673, Batch: 301/440\n",
      "Loss: 0.06684073805809021, Batch: 321/440\n",
      "Loss: 0.0014752167044207454, Batch: 341/440\n",
      "Loss: 0.0008325593662448227, Batch: 361/440\n",
      "Loss: 0.036015238612890244, Batch: 381/440\n",
      "Loss: 0.05677507817745209, Batch: 401/440\n",
      "Loss: 0.003893214510753751, Batch: 421/440\n",
      "Epoch : 4...440\t\t\n",
      "Loss: 0.08589917421340942, Batch: 1/440\n",
      "Loss: 0.28925850987434387, Batch: 21/440\n",
      "Loss: 0.23754429817199707, Batch: 41/440\n",
      "Loss: 0.12782785296440125, Batch: 61/440\n",
      "Loss: 0.06232990697026253, Batch: 81/440\n",
      "Loss: 0.011397013440728188, Batch: 101/440\n",
      "Loss: 0.13206006586551666, Batch: 121/440\n",
      "Loss: 0.05131572112441063, Batch: 141/440\n",
      "Loss: 0.0066465954296290874, Batch: 161/440\n",
      "Loss: 0.0065749757923185825, Batch: 181/440\n",
      "Loss: 0.051124464720487595, Batch: 201/440\n",
      "Loss: 0.03198425844311714, Batch: 221/440\n",
      "Loss: 0.09764530509710312, Batch: 241/440\n",
      "Loss: 0.031730715185403824, Batch: 261/440\n",
      "Loss: 0.11804630607366562, Batch: 281/440\n",
      "Loss: 0.1373763531446457, Batch: 301/440\n",
      "Loss: 0.15786421298980713, Batch: 321/440\n",
      "Loss: 0.055407311767339706, Batch: 341/440\n",
      "Loss: 0.03361622989177704, Batch: 361/440\n",
      "Loss: 0.056088536977767944, Batch: 381/440\n",
      "Loss: 0.019511757418513298, Batch: 401/440\n",
      "Loss: 0.1002209261059761, Batch: 421/440\n",
      "Batch : 440/440\t\t\r"
     ]
    }
   ],
   "source": [
    "total_batches = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    _total_correct = 0\n",
    "    \n",
    "    print(f\"Epoch : {epoch}...\")\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        print(f\"Batch : {batch_idx + 1}/{total_batches}\\t\\t\", end=\"\\r\")\n",
    "        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        preds = model(data)\n",
    "        \n",
    "        # print(preds.shape, label.shape)\n",
    "        # print(label)\n",
    "        \n",
    "        loss = criterion(preds, label.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Loss: {loss.item()}, Batch: {batch_idx + 1}/{total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1a098b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:24:03.667669Z",
     "iopub.status.busy": "2024-03-18T13:24:03.667298Z",
     "iopub.status.idle": "2024-03-18T13:24:03.708897Z",
     "shell.execute_reply": "2024-03-18T13:24:03.707912Z"
    },
    "papermill": {
     "duration": 0.140467,
     "end_time": "2024-03-18T13:24:03.711313",
     "exception": false,
     "start_time": "2024-03-18T13:24:03.570846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcffba4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:24:03.920564Z",
     "iopub.status.busy": "2024-03-18T13:24:03.920239Z",
     "iopub.status.idle": "2024-03-18T13:24:05.640962Z",
     "shell.execute_reply": "2024-03-18T13:24:05.640021Z"
    },
    "papermill": {
     "duration": 1.833851,
     "end_time": "2024-03-18T13:24:05.646746",
     "exception": false,
     "start_time": "2024-03-18T13:24:03.812895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_randomly_form_dataset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 8\u001b[0m, in \u001b[0;36mplot_randomly_form_dataset_model\u001b[1;34m(__dataset, __model)\u001b[0m\n\u001b[0;32m      6\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m __dataset[index]\n\u001b[0;32m      7\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m     10\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(__dataset\u001b[38;5;241m.\u001b[39mlabel_id2str(label\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m __dataset\u001b[38;5;241m.\u001b[39mlabel_id2str(result\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m     12\u001b[0m plot_image_from_list(images, labels, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 8\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 317\u001b[0m, in \u001b[0;36mMobileNetV4.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 317\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x0)\n\u001b[0;32m    319\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x1)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "plot_randomly_form_dataset_model(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6311477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\urp_src\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78452d7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[64, 240, 320, 3] to have 1 channels, but got 240 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate accuracy on the test dataset\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the model on the test dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     11\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move data to the device\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 8\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 317\u001b[0m, in \u001b[0;36mMobileNetV4.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 317\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x0)\n\u001b[0;32m    319\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x1)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\최상원\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[64, 240, 320, 3] to have 1 channels, but got 240 channels instead"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"model.pt\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the device\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(labels)):\n",
    "                if (predicted[i] == labels[i]):\n",
    "                    correct += 1\n",
    "            \n",
    "           \n",
    "            print(labels.squeeze())\n",
    "            print(\"predicted = {},\\n total  = {}, correct = {}\".format(predicted,total,correct))\n",
    "            \n",
    "            # print(outputs)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f'Accuracy of the model on the test dataset: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da7c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0ab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a410e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fb0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46021b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 75548,
     "sourceId": 170620,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 824.711442,
   "end_time": "2024-03-18T13:24:09.430326",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-18T13:10:24.718884",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
